[
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Jumpstart your cloud career with AWS SimuLearn by Denee McCloud, Hetvi Parsana, Karishma Damania, and Kattie Sepehri | on 25 JUN 2025 | in Amazon API Gateway, Amazon Bedrock, Amazon DynamoDB, Amazon SageMaker, AWS Config, AWS Lambda, Best Practices, Generative AI, Healthcare | Permalink | Share\nFor early-career cloud professionals, gaining hands-on experience with real customer engagements can be challenging. Even when shadowing opportunities exist, the fast-paced nature of these interactions rarely allows time to pause, reflect, and truly understand complex concepts.\nAWS SimuLearn addresses this gap by combining generative AI-driven customer simulations with practical technical training. Powered by Amazon Bedrock, AWS SimuLearn provides an immersive, risk-free environment where you can develop both technical and soft skills through interactive customer conversations, solution concept videos, hands-on labs, and practical exercises—all at your own pace.\nIn this post, we’ll follow three early-career AWS Solutions Architects who used these role-based learning plans and simulations to enhance their cloud expertise and customer-facing readiness. Their experiences demonstrate how structured practice in a controlled environment can accelerate professional development.\nHetvi’s story: Role-based learning focused on generative AI As a new Solutions Architect, I faced a common challenge: how to effectively communicate complex technical solutions to business leaders. While I was confident in my technical knowledge, translating cloud concepts into business value proved to be a different skill altogether.\nAWS SimuLearn transformed my approach to customer conversations. Through interactive simulations, I practiced engaging with virtual customers who needed generative AI solutions for their business challenges.\nOne memorable scenario involved helping a retail client understand how AI could enhance their customer experience through personalized shopping recommendations. I was provided with immediate feedback on both my technical recommendations and communication style. I learned to replace technical jargon with business outcomes, focusing on ROI and operational improvements rather than architectural specifications.\nWhat made the experience invaluable was the ability to retry conversations and experiment with different approaches. Each iteration helped me refine my message and build confidence in handling complex customer interactions.\nToday, I can confidently bridge the gap between technical solutions and business value, a skill that’s proven essential in my role as a Solutions Architect. For anyone looking to enhance their customer communication skills in technical roles, AWS SimuLearn offers a risk-free environment to practice and grow.\nKarishma’s story: Building technical depth As a Solutions Architect, I’ve found that deep technical expertise is essential for designing effective customer solutions. To expand my technical depth, I’ve combined individual training topics in AWS SimuLearn to create a structured learning experience tailored to real-world technical challenges.\nI started with security at the edge, progressing through modules on network security, encryption, identity and access management, and finally serverless technologies like Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. Each module included simulated customer interactions where I practiced gathering requirements and proposing solutions, followed by hands-on labs to solidify my understanding.\nBy combining training modules, I was able to simulate customer-specific workflows, ensuring a deeper grasp of their challenges. This personalized approach didn’t just prepare me for customer presentations, it gave me a framework to continuously refine my technical depth. With AWS SimuLearn, I can now rapidly upskill in unfamiliar areas and confidently architect solutions that align with business objectives, making me a more effective Solutions Architect.\nKattie’s story: Industry-based learning My background is in designing and developing software tools for Health Care and Life Sciences (HCLS) research teams. While I’m familiar with several programming languages, I had limited cloud knowledge at the level of an AWS Certified Solutions Architect – Associate and wanted to expand my knowledge about how AWS can be utilized in the HCLS industry. Additionally, I wanted to gain in-depth knowledge about various AWS services, use cases, and integration with third-party software.\nThe AWS SimuLearn: Healthcare Learning Plan provided a wide range of business problems that gave me the opportunity to experiment with various AWS services to find technical solutions for the business problems. For example, I experimented with services relevant to HIPAA compliance with AWS Config and AWS Systems Manager and AI with Amazon SageMaker, as well as batch processing, analytics, database and storage, dashboards with real-time data processing, IoT, and more.\nEach learning plan can be started using the Open Dialogue mode or the Scripted mode. Initially, I chose the Scripted mode because it was less challenging, but once I observed a few different simulated conversations, I felt comfortable using the Open Dialogue mode. Open Dialogue mode offers an opportunity to hone your soft skills through an interactive, real-time customer conversation.\nAlong the way, AWS SimuLearn provides helpful hints to guide the conversation, as well as an incomplete architecture diagram for you to expand upon during the dialogue with the customer. You also get asked questions regarding different services to further enhance the learning experience.\nGet started with AWS SimuLearn Through simulated customer conversations and hands-on learning, AWS SimuLearn helped these three early-career professionals on their learning journeys. They received real-time feedback to improve their soft and technical skills while gaining experience in a live console environment.\nWith 200+ trainings, including learning options available by cloud role or industry, AWS SimuLearn offers you a personalized learning experience. Game-based training with simulations empowers early-career professionals to build both technical expertise and customer engagement skills in a risk-free environment.\nReady to launch your cloud career? Begin with the free AWS SimuLearn: Cloud Practitioner Learning Plan and explore the full library of topic-based simulations.\nTo learn more about what’s new with AWS SimuLearn, check out this recent blog post: Introducing AWS SimuLearn: Generative AI Practitioner.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "How to Set Up Automated Alerts for Newly Purchased AWS Savings Plans by Syed Muhammad Tawha and Dan Johns | on 26 JUN 2025 | in Amazon Simple Notification Service (SNS), AWS Cloud Financial Management, AWS CloudFormation, Cloud Cost Optimization | Permalink | Share\nAs organizations expand, FinOps teams require a comprehensive overview of AWS Savings Plans commitments to maximize utilization efficiency. This solution involves implementing monitoring systems and automated alerts to identify underutilized Savings Plans within the eligible return period.\nWhen you purchase a Savings Plan, you make a commitment for one or three years. Savings Plans with an hourly commitment of $100 or less can be returned if they were purchased within the last seven days and in the same calendar month, provided you haven’t reached your return limit. Once the calendar month ends (UTC time), these purchased Savings Plans cannot be returned.\nIn this blog post, we provide AWS CloudFormation templates that create AWS Step Functions state machine, Amazon Simple Notification Service (SNS) topic, Amazon EventBridge scheduler, and necessary AWS Identity and Access Management (IAM) roles to automate the monitoring of newly purchased Savings Plans and highlight those that are underutilized.\nOverview of Solution: This solution follows AWS security best practices by separating the deployment across two accounts. One CloudFormation stack will be created in the Management account to establish necessary IAM roles for fetching Savings Plans utilization. Another CloudFormation stack will be deployed in your chosen Member Account within your AWS Organization.\nThe CloudFormation stack in a member account creates a state machine that assumes a role in your management account and analyzes all Savings Plans in your management account, including those purchased across your organization. The workflow filters active Savings Plans based on their purchase date, focusing specifically on plans acquired within the last 7 days and the current calendar month. It then evaluates their utilization rates and identifies plans falling below the defined threshold.\nThe state machine executes at your specified frequency and uses Amazon SNS to send email alerts to addresses you provide during CloudFormation stack creation. These alerts contain detailed information about low-utilization Savings Plans and instructions for the return process.\nFigure 1: AWS architecture diagram – Member account assumes a role to read Savings Plans data from the management account and triggers a Step Function, which sends email alerts via SNS. Solution Walk Through: Prerequisites An AWS account IAM permissions to create a CloudFormation Stack and deploy an IAM role in the management Account IAM permissions to create a CloudFormation Stack and deploy Step Functions, IAM roles, SNS, and EventBridge scheduler in your chosen member Account Deploy the solution In this section we will deploy resources for this solution in your accounts:\nPart 1 – Member Account Deployment In this section, we will deploy resources for this solution in your chosen member account.\nLogin to your AWS Management Console of the member account where you want this solution to run Deploy this CloudFormation Stack Provide the Stack Name as new-sp-utilization-alert-member In the AlertEmails parameter, enter a comma-separated list of email addresses that will receive notifications about underutilized Savings Plans. In the ManagementAccountId parameter, enter the 12 digit AWS Account Id of your AWS management account. In the ScheduleExpression parameter, specify the execution frequency for the Step Functions state machine using cron format (default is daily at 9 AM UTC). In the UtilizationThreshold parameter, specify the minimum utilization percentage for your Savings Plans. You receive alerts when utilization falls below this threshold. Click Next, select the acknowledgment box, and create the stack Wait until the stack has finished deploying and is showing as CREATE-COMPLETE You will receive an email to confirm your subscription to the SNS topic created by this stack. Please confirm the subscription to begin receiving notifications. Visit the Outputs tab of the stack you just created and make a note of the values of the ExecutionRoleArn and StateMachineArn Keys, you will need these in the next part. Part 2 – Management Account Deployment Log in to your AWS Management Console. Note: This must be the same account as the one entered in the ManagementAccountId parameter in the previous part. Deploy this CloudFormation stack Provide the Stack Name as new-sp-utilization-alert-management In the ExecutionRoleArn parameter, provide the value copied from the stack outputs of the stack deployed in the member account. In the StateMachineArn parameter, provide the value copied from the stack outputs of the stack deployed in the member account. Click Next, select the acknowledgment box, and create the stack Wait until the stack has finished deploying and is showing as CREATE-COMPLETE Test the Solution Now that the Step Functions state machine and associated resources are deployed in your member account, let’s test the deployment:\nLogin back in to your AWS Management Console of the member account where you deployed part 1 of this solution. Navigate to the Resources tab in your CloudFormation stack and locate the SavingsPlansAlerts Step Functions state machine. Click the blue hyperlink. You will be redirected to the Step Functions console. Click the Start execution button on the right. View the execution details in the Events section to monitor the state machine’s progress. If you have any Savings Plans purchased within the last 7 days and the current calendar month, you will receive email notifications. A successful execution is indicated by a green box in the Graph view. If any Savings Plans fall below your specified utilization threshold, you will receive an email at your provided address. Clean Up All resources deployed for this solution can be removed by deleting the CloudFormation stacks. You can delete the stack through either the AWS Management Console or the AWS CLI.\nTo delete the management account stack (CLI):\naws cloudformation delete-stack –stack-name new-sp-utilization-alert_management To delete the member account stack (CLI):\naws cloudformation delete-stack –stack-name new-sp-utilization-alert_member Understanding Alerts and Taking Action When you receive an alert about underutilized Savings Plans, you should review the utilization details provided in the email notification. Analyze your utilization metrics against the original commitment you made when purchasing the Savings Plan, and investigate whether the low utilization is an expected or due to other factors such as workload migration, architectural changes, or miscalculated capacity needs. Consider returning the Savings Plan if the utilization remains consistently below your threshold, the plan was purchased within the last 7 days, the purchase occurred in the current calendar month, and the hourly commitment is $100 or less. Document the return reason for future reference and planning.\nConclusion In this post, we explored how to use the Savings Plan and Cost Explorer APIs to identify underutilized Savings Plans in your organization. We then demonstrated how to use a Step Functions State Machine to filter Savings Plans purchased within the last 7 days and the current calendar month. This timing is crucial because you can return Savings Plans within the return window if they were purchased inadvertently or aren’t being utilized effectively. For guidance on returning a purchased Savings Plan, please refer to the Returning a Purchased Savings Plan documentation.\nSyed Muhammad Tawha\nSyed Muhammad Tawha is a Principal Technical Account Manager at AWS based in Dublin, Ireland. Tawha specializes in Storage, Resilience and Cloud Cost Optimization. He is passionate about helping AWS customers. Tawha also loves spending time with his friends and family. Dan Johns\nDan Johns is a Senior Solutions Architect Engineer, supporting his customers to build on AWS and deliver on business requirements. Away from professional life, he loves reading, spending time with his family and automating tasks within their home. "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Operating BYOL Windows Server Workloads Effectively on AWS by Ali Alzand, Jon Madison, and Mike Gupta | on 01 JUL 2025 | in Amazon EC2, AWS Config, AWS Cost and Usage Report, AWS License Manager, AWS Migration Hub, Technical How-to, Windows on AWS | Permalink | Share\nOne way that customers running Microsoft Workloads on Amazon Web Services (AWS) may reduce costs is taking advantage of Bring Your Own License (BYOL) for eligible licenses they own. In this blog post, we are going to share a few practices to help you optimize your operation of BYOL Windows Server workloads on AWS.\nIntroduction A common way to run your Windows Server workloads on Amazon Elastic Compute Cloud (Amazon EC2) is to use the “license included” option. This has the benefit of not having to purchase or manage your own licenses and the flexibility of per-second billing. However, if you have already purchased licenses and they are eligible for use on AWS, then it makes sense to bring them and reduce your costs accordingly.\nWe will review several specific techniques to help you when running BYOL Windows Server workloads on AWS. They are:\nPreparing your on-premises servers for import to AWS as Amazon Machine Images (AMIs). Transition and manage your Windows licenses from BYOL to license included when appropriate. Detecting configuration issues using an AWS Config custom rule. Understanding data related to your BYOL Windows instances in the AWS Cost and Usage Report (AWS CUR). BYOL for Windows on AWS To take advantage of BYOL, you need to confirm that they are eligible. AWS provides guidance for Microsoft Licensing on AWS. When determining if your Windows licenses are eligible for BYOL on AWS, consider:\nLicenses must be perpetual, and purchased before October 1, 2019, or as a true-up on an Enterprise Agreement (EA) that was active at that time. The Windows version must be Windows Server 2019 or earlier. If your licenses are eligible, then you can use them on AWS. Regardless of whether or not you have Software Assurance on your licenses, Windows Server is not eligible for License Mobility. This means that the licenses will need to apply to hardware dedicated to you alone. Amazon EC2 Dedicated Hosts are a solution that fulfills this requirement. Dedicated Hosts provide you with a familiar experience for running your Amazon EC2 instances, without the need to manage hardware or a hypervisor. AWS License Manager is a service used to manage licenses in AWS, and it is key to an effective BYOL Windows strategy.\nThe billing for your Amazon EC2 Windows instances is determined from the usage operation field that the instance inherits from its source AMI. Windows instances that run with the license included, regardless of tenancy, use the usage operation of RunInstances:0002. However, when you use your own license for a Windows instance on dedicated hosts, the usage operation of RunInstances:0800 is required. The how to create an Amazon EC2 AMI usage and billing information report blog post will help you generate the usage operation for the instances in your organization.\nPreparing your images for BYOL One requirement for using your own Windows licenses on AWS is to supply your own AMI, rather than using one created by AWS. When bringing your own image to AWS, you have different options to produce them. If the destination for your Windows server is BYOL on dedicated hosts, these tools will help you ensure your AMI is ready for use.\nVM Import/Export (VMIE) is a tool that helps you to import virtual machine images from your existing virtualization platform as Amazon Machine Images. The first step is to export your virtual machine using a standard format such as Open Virtual Appliance (OVA), ESX Virtual Machine Disk (VMDK), or Virtual Hard Disk (VHD/VHDX). Then, upload the image to an Amazon Simple Storage Service (S3) bucket in anticipation of the conversion process.\nTo use VMIE, use these instructions to create an AWS Identity and Access Management (IAM) role named “vmimport” that the service will use to perform operations on your behalf.\nWhen using the AWS Command Line Interface (AWS CLI) to import a Windows image that you are planning on using for BYOL on dedicated hosts, it is necessary to specify the license type to set the usage operation correctly on the resultant AMI. To import an image, a command such as the following can be used (in this case for an OVA image in an S3 bucket):\naws ec2 import-image –usage-operation RunInstances:0800 –disk-containers Format=OVA,Url=s3://\u0026lt;\u0026lt;my-bucket\u0026gt;\u0026gt;/\u0026lt;\u0026lt;my-image-name\u0026gt;\u0026gt;.ova This will start an import job that, once completed, will yield an AMI with the proper usage code for Windows BYOL.\nMigration Hub Orchestrator is a tool that lets you create workflows to automate tasks and simplify the migration process. One of the workflow templates that Orchestrator provides is “Import virtual machine images to AWS”. Use this workflow to import an image for Windows BYOL.\nOpen the AWS console and navigate to the Migration Hub Console. Choose Workflows in the Orchestrate side menu. Choose Create Workflow (Figure 1) Figure 1: Create Workflow Select the Import virtual machine images to AWS template (Figure 2) and choose Next Figure 2: Select the import virtual machine template On the Configure your workflow page, enter a Name for the workflow, and optionally enter a Description. In the Source environment configuration section, populate the Disk container field, which is the S3 bucket where you stored your image from on premises. The name must conform to the requirements from the Migration Hub Orchestrator documentation. Figure 3: Configure source environment In the Target environment configuration section, select the operating system and license for the virtual machines created with the resultant AMI. Choose Windows Server BYOL without SQL Server. Figure 4: Choose the licensing model Use the rest of the fields to further customize your AMI based on your requirements. These include the boot mode, AWS Key Management Service (KMS) encryption key, tags and license specification (for business case analysis). You also have the option to leave these with their default values. Choose On the Review and submit page, choose After uploading an image and creating your workflow, it is ready to run by choosing Run workflow.\nFigure 5: Run Workflow Managing license conversion properly There are scenarios in which you will need to switch Amazon EC2 instances from the BYOL licensing model to license included and vice versa. These include (but are not limited to):\nUpgrading the operating system of the Amazon EC2 instance to Windows Server 2022, which is not eligible for BYOL, regardless of tenancy. Moving an Amazon EC2 instance off a dedicated host to run it on shared tenancy EC2, which is not eligible for BYOL. Moving an Amazon EC2 instance that is eligible for BYOL from shared tenancy to a dedicated host. When you need to switch the licensing model of an Amazon EC2 instance, use the License type conversion feature in AWS License Manager. License type conversion lets you change the usage operation. See our guide for eligible license types for Windows and SQL Server in License Manager.\nDetecting configuration issues with AWS Config AWS Config is a service that helps you assess, audit, and evaluate the configuration of your AWS resources. By leveraging a custom AWS Config Rule, you can detect potential license misconfiguration in instances running on dedicated hosts, saving unnecessary licensing costs.\nThe aws-config-rules repository contains custom AWS Config Rules to deploy to your AWS account using the AWS Config Rules Development Kit (RDK). Use the custom AWS Config Rule called EC2_INSTANCE_LICENSE_INCLUDED_DEDICATED_HOST to detect instances with license-included Windows Server (usage operation RunInstances:0002) running on Dedicated Hosts.\nUse AWS CloudShell to run the RDK and test the AWS Config rules deployment. To install the custom rule, open CloudShell in the AWS Console in the desired AWS Region, and run the following commands:\npip install rdk rdk init git clone https://github.com/awslabs/aws-config-rules cd aws-config-rules/python rdk deploy EC2_INSTANCE_LICENSE_INCLUDED_DEDICATED_HOST Once the rule has completed deployment, view the rule in the AWS Config console. For instances with mis-configured licenses, either move them to Shared tenancy or follow the License Conversion process accordingly.\nFigure 6: Custom Config rule Understanding CUR data for BYOL instances AWS Cost and Usage Reports (CUR) contains the most comprehensive set of cost and usage data available. Use Amazon Athena to query your CUR data. The following query shows the licenses your instances are being billed for:\nselect line_item_resource_id, line_item_operation, line_item_line_item_type, month, year, line_item_unblended_cost, line_item_blended_cost, line_item_usage_type, line_item_usage_account_id, line_item_line_item_description from customer_all where line_item_usage_account_id = \u0026#39;[ACCOUNT NUMBER]\u0026#39; and line_item_line_item_type = \u0026#39;Usage\u0026#39; and line_item_operation like \u0026#39;%RunInstances:%\u0026#39; Based on the results of the above Query, the line_item_operation field shows what you’re being billed for.\nFigure 7: AWS CUR output Conclusion Implementing BYOL for Windows Server workloads on AWS successfully, requires careful attention to license eligibility, configuration, and ongoing management. By understanding the key requirements – from license purchase dates and Windows Server versions to proper usage operation codes on dedicated hosts – organizations can effectively reduce their cloud infrastructure costs while maintaining compliance. Success depends on three key elements:\nProper license evaluation – identifying eligible licenses based on purchase date and Windows Server version Accurate configuration – ensuring correct usage operation codes to avoid double-billing on dedicated hosts Ongoing monitoring – maintaining regular assessment of usage and costs By following these practices, organizations can optimize their Windows Server deployment costs while maintaining licensing compliance on AWS.\nReady to start optimizing your Windows Server costs on AWS? Request an AWS Optimization and Licensing Assessment to begin evaluating your licensing opportunities and potential cost savings.\nAWS has significantly more services, and more features within those services, than any other cloud provider, making it faster, easier, and more cost effective to move your existing applications to the cloud and build nearly anything you can imagine. Give your Microsoft applications the infrastructure they need to drive the business outcomes you want. Visit our .NET on AWS and AWS Database blogs for additional guidance and options for your Microsoft workloads. Contact us to start your migration and modernization journey today.\nTAGS: Amazon EC2, AWS License Manager, Cost Savings, Microsoft, Windows On AWS, Windows Server\nAli Alzand\nAli is a Microsoft Specialist Solutions Architect at Amazon Web Services who helps global customers unlock the power of the cloud by migrating, modernizing, and optimizing their Microsoft workloads. He specializes in cloud operations — leveraging AWS services like Systems Manager, Amazon EC2 Windows, and EC2 Image Builder to drive cloud transformation. Outside of work, Ali enjoys exploring the outdoors, firing up the grill on weekends for barbecue with friends, and sampling all the eclectic food has to offer. Jon Madison\nJon Madison is a Pr. Delivery Consultant on the AWS Professional Services (ProServe) Energy Team. He has a background in Cloud Infrastructure, Security, and DevOps, and is passionate about helping customers with cloud adoption and building scalable solutions and processes. In his free time Jon enjoys cooking, gaming, and spending time with his family and friends. Mike Gupta\nMike Gupta is a Senior Technical Account Manager at AWS based out of New York City. In his role, he provides strategic technical guidance to help customers use AWS best practices to plan and build solutions. He’s dedicated to empower customers to develop scalable, resilient, and cost-effective architectures. In his free time, Mike enjoys spending time with his wife and family, exploring local history and trying new restaurants. "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.3-set-up-amazon-dynamodb/5.3.1-create-tables/",
	"title": "Create tables",
	"tags": [],
	"description": "",
	"content": " Open Management Console, search and choose DynamoDB. Navigate to tab Tables, click Create table For Table name, enter SmartOffice_RoomLog_Prod For Partition key, enter roomId For Sort Key, enter timestamp For Table settings, choose Customize settings For Table class, choose DynamoDB Standard-IA Log sensor with less access, need to optimize storage pricing.\nLeave other setting at default Add Tag for management (Key: Project, Value: SmartOffice; Key: Environment, Value: Dev) Click Create table Do the same for table: SmartOffice_User_Prod Name: SmartOffice_User_Prod Partition key: userId Table settings: Customize settings Deletion protection: Check Turn on deletion protection Tag : Key: Project, Value: SmartOffice; Key: Environment, Value: Dev SmartOffice_Office_Prod Name: SmartOffice_Office_Prod Partition key: officeId Table settings: Customize settings Deletion protection: Check Turn on deletion protection Tag : Key: Project, Value: SmartOffice; Key: Environment, Value: Dev SmartOffice_RoomConfig_Prod Name: SmartOffice_RoomConfig_Prod Partition key: roomId Table settings: Customize settings Deletion protection: Check Turn on deletion protection Tag : Key: Project, Value: SmartOffice; Key: Environment, Value: Dev "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.4-set-up-amazon-cognito/5.4.1-create-user-pool/",
	"title": "Create user pool",
	"tags": [],
	"description": "",
	"content": " Open Management Console, search and choose Cognito. In tab User pool, click Create user pool For Name your application, enter SmartOffice For Application type, choose Single-page application (SPA) For Options for sign-in identifiers, check Email For Self-registration, uncheck Enable self-registration For Required attributes for sign-up, check email "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: “AI-Driven Development Life Cycle: Reimagining Software Engineering” Event Objectives Explores AI-Driven Development using Amazon Q Developer Explores Spec-Driven Development using Kiro Speakers Toan Huynh – SA My Nguyen – ISV Account Director Key Highlights The Role of AI in IT 2023: AI acted as an assistant, helping developers minimize workload, recommend code, and check errors 2024: AI became embedded in IDEs, enabling system design and automated workflows 2025: Software engineers with strong AI skills can develop products in minimal time Developing Products with AI While Keeping Humans at the Center Developers can use AI to generate code but they still must validate its correctness and effectiveness The owner of AI-generated code is the developer, since they are responsible for validating it before implementation Coding with AI usually leads to two paths: AI-Assistant: Developers see AI as a co-worker who suggests code and solutions AI-Manager: AI takes full responsibility for designing systems, generating and implementing code, and maintaining the product AI-Driven Development Lifecycle (AI-DLC) AI acts as an assistant that helps to develop the requirements and specifications AI supports developers by writing code in small units step-by-step, waiting for validation before moving on to the next task It requires developers to minimize the scope and provide a specified prompt for AI Mob programming is recommended for this methodology since it reduces product development time Spec-Driven Development Lifecycle with Kiro Kiro is an IDE that supports developers by taking specifications and developing step-by-step tasks to implement them into code Kiro provides a log of all the actions and prompts, allowing developers to track progress Kiro is suitable for projects that need to have accurate specifications before starting to write code Key Takeaways Developer Mindset Human-centric: AI can design systems and write code, but developers remain responsible for validating the results Ready for changes: With powerful AI tools like Amazon Q Developer and Kiro emerging, developers should continuously prepare for new technologies Pros and Cons Using Ai-Driven or Spec-Driven Development Lifecycle depends on the scope and purpose of the project Each method has pros and cons that affect the development team as well as the product Pros AI-Driven Development Lifecycle is good for projects that need the speed, reusability, cost efficiency, and scalability Spec-Driven Development Lifecycle is good for projects that need narrative quality, consistency, and predictability Cons AI-Driven Development Lifecycle may result in inconsistent quality, ethical/IP concerns, or maintenance challenges Spec-Driven Development Lifecycle may result in slower production cycles, higher costs, or limited content Applying to Work Apply AI-DLC or SDD to current projects Learn new technologies like Amazon Q Developer or Kiro Event Experience Attending the “AI-Driven Development Life Cycle: Reimagining Software Engineering” workshop was extremely valuable. I had the opportunity to learn new methodologies and connect with IT professionals. Key experiences included:\nLearning from highly skilled speakers Learning from experts how they handle errors in coding and product development Experiencing real-world use cases of the methodologies Leveraging modern tools Explored Amazon Q Developer, an AI tool that supports AI-DLC Observed a demonstration of how Kiro is used in the Spec-Driven Development Lifecycle Overall, the event not only provided technical knowledge but also introduced to new tools that can support development lifecycle\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: “GenAI-powered App-DB Modernization workshop” Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Tran Ngoc Khiet\nPhone Number: 0908171773\nEmail: tranngockhiet22062005@gmail.com\nUniversity: FPT University Ho Chi Minh Campus\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 06/09/2025 to 14/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.6-set-up-aws-lambda-function/5.6.1-set-up-client-aws-lambda-function/",
	"title": "Set up client AWS Lambda Functions",
	"tags": [],
	"description": "",
	"content": "Create Function Open Management Console, search and choose Lambda Navigate to tab Functions, click Create function Choose Author from scratch For Function name, enter SmartOfficeProfileUpdateHandler For Runtime, choose Python 3.14 For Architecture, choose arm64 Expand the Change default execution role, choose Use an existing role For Existing role, choose SmartOfficeProfileUpdateHandler Click Create Function In the Code source IDE, paste this code to it import boto3 import json import os # Create client and handler dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) cognito_client = boto3.client(\u0026#39;cognito-idp\u0026#39;) # Get environment variables TABLE_NAME = os.environ.get(\u0026#39;TABLE_NAME\u0026#39;) USER_POOL_ID = os.environ.get(\u0026#39;USER_POOL_ID\u0026#39;) table = dynamodb.Table(TABLE_NAME) def build_update_params(updates): update_expression = \u0026#34;SET \u0026#34; expression_names = {} expression_values = {} # Use #name and #email to avoid reserved words attribute_map = { \u0026#34;name\u0026#34;: \u0026#34;#name\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;#email\u0026#34;, } for key, value in updates.items(): if key in attribute_map: placeholder = f\u0026#34;:{key}\u0026#34; attr_name = attribute_map[key] update_expression += f\u0026#34;{attr_name} = {placeholder}, \u0026#34; expression_values[placeholder] = value expression_names[attr_name] = key update_expression = update_expression.rstrip(\u0026#34;, \u0026#34;) return update_expression, expression_names, expression_values def lambda_handler(event, context): try: if \u0026#34;body\u0026#34; in event: body = json.loads(event[\u0026#34;body\u0026#34;]) else: body = event user_id = body.get(\u0026#39;userId\u0026#39;) updates = body.get(\u0026#39;updates\u0026#39;) if not user_id or not updates: return {\u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: \u0026#39;Lỗi: Thiếu \u0026#34;userId\u0026#34; hoặc \u0026#34;updates\u0026#34;\u0026#39;} update_expr, attr_names, attr_values = build_update_params(updates) if not attr_values: return {\u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: \u0026#39;Không có trường hợp lệ nào để cập nhật\u0026#39;} dynamo_response = table.update_item( Key={\u0026#39;userId\u0026#39;: user_id}, UpdateExpression=update_expr, ExpressionAttributeNames=attr_names, ExpressionAttributeValues=attr_values, ReturnValues=\u0026#34;UPDATED_NEW\u0026#34; ) if \u0026#39;email\u0026#39; in updates: new_email = updates[\u0026#39;email\u0026#39;] try: cognito_client.admin_update_user_attributes( UserPoolId=USER_POOL_ID, Username=user_id, UserAttributes=[ { \u0026#39;Name\u0026#39;: \u0026#39;email\u0026#39;, \u0026#39;Value\u0026#39;: new_email }, { \u0026#39;Name\u0026#39;: \u0026#39;email_verified\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;true\u0026#39; } ] ) except Exception as e: print(f\u0026#34;Cognito Error: {e}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: f\u0026#39;Update DynamoDB success, but failed to update Cognito: {str(e)}\u0026#39; } success_body = { \u0026#34;message\u0026#34;: \u0026#34;Update success\u0026#34;, \u0026#34;updatedAttributes\u0026#34;: dynamo_response.get(\u0026#39;Attributes\u0026#39;) } return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(success_body) } except Exception as e: print(f\u0026#34;Unknow error: {e}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: f\u0026#39;Error when handle request: {str(e)}\u0026#39; } Click Deploy Navigate to tab Configuration, choose Environment variables Click Edit Click Add environment variable twice For first Key, enter TABLE_NAME For first Value, enter SmartOffice_User_Prod For second Key, enter USER_POOL_ID For second Value, enter your user pool ID from AWS Cognito User Pool (Go to Cognito -\u0026gt; User pools -\u0026gt; Choose your User pool -\u0026gt; Overview -\u0026gt; User pool ID) Click Save Test Function Navigate to tab Test For Event name, enter test-update-name For Event JSON, copy and paste this test json { \u0026#34;userId\u0026#34;: \u0026#34;692a45fc-0091-7055-703b-49b4db5cbe0c\u0026#34;, \u0026#34;updates\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Tên Mới\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;email.moi@example.com\u0026#34; } } Get userId from User in Amazon Cognito User Pool (Go to Cognito -\u0026gt; User pools -\u0026gt; Choose your User pool -\u0026gt; Users -\u0026gt; Choose your User -\u0026gt; User ID)\nClick Test "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.5-set-up-role-for-aws-lambda-function/5.5.1-set-up-role-for-client-aws-lambda-function/",
	"title": "Set up IAM Role for client AWS Lambda Functions",
	"tags": [],
	"description": "",
	"content": "Create Policy Open Management Console, search and choose IAM Navigate to tab Policies, click Create policy Choose JSON Enter { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCognitoUpdate\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;cognito-idp:AdminUpdateUserAttributes\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowLambdaCRUDOnUserTable\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;\u0026#34; } ] } You need to find the ARN (Amazon Resource Name), copy it and paste for each Resource - The first Resource for Amazon Cognito is your User Pool ARN (Go to Cognito -\u0026gt; User pools -\u0026gt; Choose your User pool -\u0026gt; Overview -\u0026gt; ARN) - The second Resource for Amazon DynamoDB is your DynamoDB Table ARN (Go to DynamoDB -\u0026gt; Tables -\u0026gt; Choose your Table -\u0026gt; Settings -\u0026gt; Amazon Resource Name (ARN))\nClick Next For Policy name, type SmartOfficeProfileUpdate For Description (optional), type Allow CRUD on Amazon DynamoDB and update user in Amazon Cognito Add Tag for management (Key: Project, Value: SmartOffice; Key: Environment, Value: Dev) Click Create policy Create role Navigate to tab Roles, click Create role For Trusted entity type, choose AWS service For Use case, search and choose Lambda Click Next For Permissions policies, search and check SmartOfficeProfileUpdate For Role name, enter SmartOfficeProfileUpdateHandler For Description (optional), enter Role for Lambda to perform CRUD on DynamoDB and update user on Cognito Add Tag for management (Key: Project, Value: SmartOffice; Key: Environment, Value: Dev) Click Create Role Do the same for role RoomConfig: Policy name: SmartOfficeRoomConfig Description (optional): Allow CRUD on Amazon DynamoDB Policy editor: {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;AllowLambdaCRUDOnRoomConfigTable\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;dynamodb:PutItem\u0026#34;,\r\u0026#34;dynamodb:GetItem\u0026#34;,\r\u0026#34;dynamodb:UpdateItem\u0026#34;,\r\u0026#34;dynamodb:DeleteItem\u0026#34;,\r\u0026#34;dynamodb:Query\u0026#34;,\r\u0026#34;dynamodb:Scan\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;\u0026#34;\r}\r]\r} Role Name: SmartOfficeRoomConfigHandler Description (optional), enter Role for Lambda to perform CRUD on DynamoDB Other setting same as SmartOfficeProfileUpdateHandler "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Team up for the project, get acquainted with FCJ mentor team. Watch videos from series First Cloud Journey Bootcamp - 2025 in Youtube from the begining to the end of module 2. Learn about AWS services and how to use them. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Team up and get acquainted with team members - Read rules, workshop guide and handling of violations - Watch video How to draw AWS architecture with draw.io - Watch video Guide for AWS workshop 09/08/2025 09/08/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://policies.fcjuni.com 3 - Watch theory videos of Module 1 - Watch videos, read documents of Lab 1, Lab 7 and Lab 9 - Practice: + Create AWS Free Tier Account + Config MFA for the account + Upgrade account to paid plan 09/09/2025 09/09/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000001.awsstudygroup.com https://000007.awsstudygroup.com https://000009.awsstudygroup.com 4 - Watch theory videos of Module 2 - Watch videos, read documents of Lab 3 - Practice: + Create VPC + Create Subnet + Create Route Table + Create Internet Gateway + Create NAT Gateway + Create Security Group + Create Network ACL + Create EC2 instances + Test connection between two EC2 Instances 09/10/2025 09/10/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000003.awsstudygroup.com 5 - Watch videos, read documents of Lab 10 and Lab 19 - Practice: + Create and test a connection to a Hybrid DNS with Route 53 + Create and test a VPC Peering connection between two EC2 instances 09/11/2025 09/11/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000010.awsstudygroup.com https://000019.awsstudygroup.com 6 - Watch videos, read documents of Lab 20 - Practice: + Create a Transit Gateway + Test the connection between EC2 instances through the Transit Gateway 09/12/2025 09/12/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000020.awsstudygroup.com Week 1 Achievements: Create AWS Free Tier account, upgrade to paid plan and config MFA sucessfully. Get used to AWS Management Console and add widgets to the screen. Create and config AWS services: VPC Subnet Route Table Internet Gateway NAT Gateway Security Group Network ACL EC2 Know how to create CloudFormation from templates. Know how to connect EC2 Intances through VPC Peering Connection or Transit Gateway. Know how to create and connect to DNS with Route 53. "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Watch videos from series First Cloud Journey Bootcamp - 2025 in Youtube from Module 3 to the end of module 4. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch theory videos of Module 3 - Watch videos, read documents of Lab 13 and Lab 24 - Practice: + Create and test the operation of the Backup Plan + Create and connect Storage Gateway to on-premise environment through file share 09/15/2025 09/15/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000013.awsstudygroup.com https://000024.awsstudygroup.com 3 - Watch videos, read documents of Lab 57 - Practice: + Create and config S3 Bucket + Host a static website with S3 and through CloudFront + Config bucket versioning, move object among Bucket and replicate object among multi-region Bucket 09/16/2025 09/16/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000057.awsstudygroup.com 4 - Watch theory videos of Module 4 - Watch videos, read documents of Lab 14 - Practice: + Import a Virtual Machine to AWS + Deploy an Instance from AMI + Export a Virtual Machine from an Instance 09/17/2025 09/17/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000014.awsstudygroup.com 5 - Watch videos, read documents of Lab 25 - Practice: + Create an SSD/HDD Multi-AZ file system and config File Share + Operate some functions of File Share through Remote Desktop 09/18/2025 09/18/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000025.awsstudygroup.com 6 - Learn Coursera course Research Methods and Research Methodologies 09/19/2025 09/19/2025 https://www.coursera.org/learn/research-methods https://www.coursera.org/learn/research-methodologies Week 2 Achievements: Know how to create Backup Plan Know how to create Storage Gateway and config File Share Know how to create and config S3 Bucket Know how to host a static website with S3 and through CloudFront Practice some of S3 Bucket functions: Bucket versioning Move object Replicate object Know how to import/export VM Know how to deploy an Instance from AMI Know how to operate some functions on File Share of an SSD/HDD Multi-AZ file system through CLI on a Remote Desktop "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Watch videos from series First Cloud Journey Bootcamp - 2025 in Youtube Module 5. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch theory videos of Module 5 - Watch videos, read documents of Lab 18 and Lab 22 - Practice: + Config Security Hub + Create and test Lambda Function 09/22/2025 09/22/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000018.awsstudygroup.com https://000022.awsstudygroup.com 3 - Watch videos, read documents of Lab 27 and Lab 28 - Practice: + Create, manage Tag and Resource Group + Config Policies for Role + Test access EC2 with Tag through IAM 09/23/2025 09/23/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000027.awsstudygroup.com https://000028.awsstudygroup.com 4 - Watch videos, read documents of Lab 30 and Lab 33 - Practice: + Upload and share encrypted data with S3 Bucket + Create and query Event History with Athena through CloudTrail table 09/24/2025 08/24/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000030.awsstudygroup.com https://000033.awsstudygroup.com 5 - Watch videos, read documents of Lab 44 and Lab 48 - Practice: + Config IAM Group, User and Role + Config Access Key and use IAM Role to access 09/25/2025 08/25/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000044.awsstudygroup.com https://000048.awsstudygroup.com 6 - Learn Coursera course Being a researcher (in Information Science and Technology) và Advanced Writing 09/26/2025 08/26/2025 https://www.coursera.org/learn/being-researcher https://www.coursera.org/learn/advanced-writing?specialization=academic-english Week 3 Achievements: Know how to enable and config Security Hub Know how to create and use Lambda Function Know how to mange resource with Tag and Resource Group Know how to attach Policies to Role Know how to share encrypted data through S3 Bucket Know how to query Event History with Athena through Cloudtrail table Know how to mange User through Group, Role and Access key "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Watch videos from series First Cloud Journey Bootcamp - 2025 in Youtube Module 6. Participate in AI-Driven Development Life Cycle: Reimagining Software Engineering event. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch theory videos of Module 6 09/29/2025 09/29/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 3 - Watch videos, read documents of Lab 5 - Practice: + Create RDS Database Instance + Deploy application with NodeJS 09/30/2025 09/30/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000005.awsstudygroup.com 4 - Watch videos, read documents of Lab 35 - Practice: + Create sample data and Data Stream + Analysis and visualize the Data with Athena and QuickSight 10/01/2025 10/01/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000035.awsstudygroup.com 5 - Translate Blogs: + Jumpstart your cloud career with AWS SimuLearn + Observing Agentic AI workloads using Amazon CloudWatch + Operating BYOL Windows Server Workloads Effectively on AWS 10/02/2025 10/02/2025 https://aws.amazon.com/blogs/training-and-certification/jumpstart-your-cloud-career-with-aws-simulearn/ https://aws.amazon.com/blogs/aws-cloud-financial-management/how-to-set-up-automated-alerts-for-newly-purchased-aws-savings-plans/ https://aws.amazon.com/blogs/modernizing-with-aws/operating-byol-windows-server-workloads-effectively-on-aws/ 6 - Participate in AI-Driven Development Life Cycle: Reimagining Software Engineering event - Write event report 10/03/2025 10/03/2025 Week 4 Achievements: Know how to create RDS Database Instance and deploy application through it Know how to create Data Stream, analysis and visualize data through Athena and QuickSight Translate three blogs to Vietnamese Participate in AI-Driven Development Life Cycle: Reimagining Software Engineering event: Learn about AI-Driven Deployment Lifecycle through Amazon Q Developer Learn about Spec-Driven Deployment Lifecycle through Kiro "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Watch videos from series First Cloud Journey Bootcamp - 2025 in Youtube Module 7. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch videos, read documents of Lab 39 - Practice: + Create DyanamoDB database + Operate some query of DynamoDB in AWS Cloudshell 10/06/2025 10/06/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000039.awsstudygroup.com/vi 3 - Watch videos, read documents of Lab 40 - Practice: + Build database from S3 Bucket and Glue Crawler + Analysis cost and usage performance 10/07/2025 10/07/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000040.awsstudygroup.com/vi 4 - Watch videos, read documents of Lab 60 - Practice: + Create DyanamoDB database + Operate some DynamoDB functions on management console and AWS SDK: * Create * Write * Read * Update * Query * Create Global Secondary Index * Query Global Secondary Index 10/08/2025 10/08/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000060.awsstudygroup.com/vi 5 - Watch videos, read documents of Lab 72 - Practice: + Create Firehose, IAM Role, Glue Crawler + Transform data with notebook and Glue DataBrew + Analysis data with Athena, Kinesis Data Analytics + Visualize data with QuickSight + Run Lambda Function on data + Create and connect Redshift cluster with S3 Bucket through Glue Connection 10/09/2025 10/10/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000072.awsstudygroup.com/vi 6 - Watch videos, read documents of Lab 73 - Practice: + Upload dataset + Create visual chart and table from data + Create backup dashboard + Publish dashboard 10/10/2025 10/10/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000073.awsstudygroup.com/vi Week 5 Achievements: Know how to create DynamoDB database know how to query data using AWS CloudShell, AWS SDK or management console Know how to build database from S3 Bucket and Glue Crawler Know how to analysis cost and usage performance of Glue database using Athena Apply knowlegde of Kinesis, IAM, Glue, Athena, QuickSight, Lambda, Redshift to do a workshop Know how to create visual chart and table from data with Quicksight Know how to create and publish dashboard with QuickSight "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Synthesize knowledge about AWS to mindmap for mid-term test Brainstorm idea for workshop Draw architect diagram for workshop Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Synthesize theory knowledge of Module 1 and Module 2 - Draw mindmap for Module 1 and Module 2 10/13/2025 10/13/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://drive.google.com/file/d/1jlnUWvbaxQmmhaDT4eMMyFaZNcN3qlxt/view?usp=drive_link 3 - Synthesize theory knowledge of Module 3 and Module 4 - Draw mindmap for Module 3 and Module 4 10/14/2025 10/14/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://drive.google.com/file/d/1jlnUWvbaxQmmhaDT4eMMyFaZNcN3qlxt/view?usp=drive_link 4 - Synthesize theory knowledge of Module 5 and Module 6 - Draw mindmap for Module 5 and Module 6 10/15/2025 10/15/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://drive.google.com/file/d/1jlnUWvbaxQmmhaDT4eMMyFaZNcN3qlxt/view?usp=drive_link 5 - Brainstorm idea for workshop 10/16/2025 10/16/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing 6 - Draw architect diagram for workshop 10/17/2025 10/17/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing Week 6 Achievements: Have a mindmap to summarize knowledge of AWS Brainstorm idea for workshop Draw architect diagram of workshop "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Caculate pricing and estimate Adjust architect diagram Write proposal Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Estimate and caculate price for: emsp; - Amazon DynamoDB emsp; - AWS Lambda emsp; - AWS iot core 10/20/2025 10/21/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing 3 - Estimate and caculate price for: emsp; - Amazon API Gateway emsp; - AWS Simple Cloud Storage emsp; - Amazon CloudFront 10/21/2025 10/22/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing 4 - Estimate and caculate price for: emsp; - Amazon EventBridge emsp; - Amazon SQS emsp; - Amazon CloudWatch 10/22/2025 10/23/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing 5 - Estimate and caculate price for: emsp; - Amazon Simple Notification Service emsp; - Amazon Cognito emsp; - Create Estimate for workshop 10/23/2025 10/24/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing 6 - Adjust architect diagram and write proposal 10/24/2025 10/25/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing Week 7 Achievements: Finish pricing and estimate Finish architect diagram Finish proposal "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Doing workshop for Amazon DynamoDB and Amazon Cognito Review knowledge for mid-term test Taking mid-term test Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Doing workshop for Amazon DynamoDB 10/27/2025 10/27/2025 3 - Doing workshop for Amazon Cognito 10/28/2025 10/28/2025 4 - Review mid-term test content 10/29/2025 10/29/2025 5 - Doing quiz created by NotebookLM for mid-term test 10/30/2025 10/30/2025 6 - Take mid-term test 10/31/2025 10/31/2025 Week 8 Achievements: Finish configuring Amazon DynamoDB for workshop Finish configuring Amazon Cognito for workshop Finish mid-term test "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Learn Python and Linux Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Module 1 and Module 2 - Take note of code syntax and knowlegde 11/03/2025 11/03/2025 https://www.udemy.com/course/python-mastery-the-complete-web-programming-course/?couponCode=KEEPLEARNING https://programming-note-fawn.vercel.app 3 - Learn Module 3 and Module 4 - Take note of code syntax and knowlegde 11/04/2025 11/04/2025 https://www.udemy.com/course/python-mastery-the-complete-web-programming-course/?couponCode=KEEPLEARNING https://programming-note-fawn.vercel.app 4 - Learn Module 5 and Module 6 - Take note of code syntax and knowlegde 11/05/2025 11/05/2025 https://www.udemy.com/course/python-mastery-the-complete-web-programming-course/?couponCode=KEEPLEARNING https://programming-note-fawn.vercel.app 5 - Learn Module 7 and Module 8 - Take note of code syntax and knowlegde 11/06/2025 11/06/2025 https://www.udemy.com/course/python-mastery-the-complete-web-programming-course/?couponCode=KEEPLEARNING https://programming-note-fawn.vercel.app 6 - Install and learn some basic code syntaxes of Linux Ubuntu on Virutal Box - Take note of code syntax and knowlegde 11/07/2025 11/07/2025 https://www.youtube.com/watch?v=lmeDvSgN6zY https://www.youtube.com/watch?v=I4EWvMFj37g https://programming-note-fawn.vercel.app Week 9 Achievements: Know some codde syntax of Python Know some codde syntax of Bash in Linux Ubuntu Summarize knowledge to a note website "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Team up for project, learn Module 1 and 2\nWeek 2: Learn Module 3 and 4\nWeek 3: Learn Module 5\nWeek 4: Learn Module 6 and participate in an event\nWeek 5: Learn Module 7\nWeek 6: Create a mindmap to summarize knowleadge, brainstorm idea for workshop and draw architect diagram\nWeek 7: Caculate pricing and adjust architect diagram\nWeek 8: Week 9: Week 10: Week 11: Week 12: "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.4-set-up-amazon-cognito/5.4.2-config-app-client/",
	"title": "Config app client",
	"tags": [],
	"description": "",
	"content": " In tab User pools, choose the pool you have created Navigate to tab App Clients, choose SmartOffice Click Edit For Authentication flows: - Uncheck Choice-based sign-in: ALLOW_USER_AUTH - Uncheck Sign in with username and password: ALLOW_USER_PASSWORD_AUTH - Uncheck Sign in with secure remote password (SRP): ALLOW_USER_SRP_AUTH - Check Sign in with server-side administrative credentials: ALLOW_ADMIN_USER_PASSWORD_AUTH - Uncheck Sign in with custom authentication flows from Lambda triggers: ALLOW_CUSTOM_AUTH - Check Get new user tokens from existing authenticated sessions: ALLOW_REFRESH_TOKEN_AUTH Leave other settings at default Click Save changes "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.3-set-up-amazon-dynamodb/5.3.2-config-tables/",
	"title": "Config tables",
	"tags": [],
	"description": "",
	"content": "Config TTL for table SmartOffice_RoomLog_Prod In tab Tables, choose SmartOffice_RoomLog_Prod Navigate to tab Settings, in the field Time to Live (TTL) click Turn on For TTL attribute name, enter expireAt For Simulated date and time, choose Epoch time value, enter 1770000000 Click turn on TTL Config Point-in-time recovery (PITR) for table SmartOffice_User_Prod, SmartOffice_Office_Prod and SmartOffice_RoomConfig_Prod In tab Tables, choose SmartOffice_User_Prod Navigate to tab Backups, in the field Point-in-time recovery (PITR), click Edit Check Turn on point-in-time recovery For Backup recovery period choose 35 days Click Save changes Do the same for table SmartOffice_Office_Prod and SmartOffice_RoomConfig_Prod Config DynamoDB stream for table SmartOffice_RoomConfig_Prod In tab Tables, choose SmartOffice_RoomConfig_Prod Navigate to tab Exports and streams, in the field DynamoDB stream details, click Turn on For View type, choose New and old images Click Turn on stream "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "IoT Weather Platform for Lab Research A Unified AWS Serverless Solution for Real-Time Weather Monitoring 1. Executive Summary The Smart Office Management Console is designed base on the idea of weekly trip to AWS office at Ho Chi Minh city. It supports office with smart device like auto light, auto scensor window, auto air conditioner by collect, transmit and process data from IoT Sensor. By utilizing AWS Serverless services, data can transmit via MQTT and process every 5 minutes to support real-time monitoring, acting base on predictive weather and saving cost from unused devicesm with restricted to athenticated members via Amazon Cognito\n2. Problem Statement What’s the Problem? Nowadays, offices require manual control for light, window, air conditioner, .etc. Most office turn on light and air conditioner thoughout there work hour (usually 8 a.m to 5 p.m), but this may not be necessary. For example, sunlight at 8 a.m may not be bright enough for worker to see clearly, but if the office is not very big, sunlight from 10 a.m to 3.pm can support light up very much. Additionally, in rainy season, some places have high humidity which can provide a cool environment similar to air conditioner environment. If we can control devices base on these condition, it will save energy for the world as well as money for organization.\nThe Solution The platform uses AWS IoT Core to ingest MQTT data, AWS Lambda and API Gateway for processing, DynamoDB for data storage, Amazon S3 for static website hosting and Amazon Cognito ensures secure access, AWS SNS to send notifications when system monitoring device or when supisious weather condition detect. Similar to Thingsboard and CoreIoT, users can register new devices and manage connections, though this platform operates on a smaller scale and is designed for private use. Key features include real-time monitoring, weather prediction, auto controlling and low operational costs.\nBenefits and Return on Investment The Smart Office system enhances automation, monitoring, and energy efficiency across office environments, providing a reliable platform for research, expansion, and real-world IoT application development. It serves as a foundational resource for lab members and developers to build advanced smart environment solutions, while offering a centralized system that reduces manual configuration, improves data reliability, and simplifies maintenance.\nMonthly operating costs are estimated at $1.81 USD, including DynamoDB ($0.09), IoT Core ($0.18), CloudFront ($1.27), CloudWatch ($0.25) and SNS ($0.02), totaling $21.72 USD per year. As IoT hardware is already deployed, there are no additional equipment expenses. The system achieves a break-even period of 6–12 months through significant time savings and reduced manual operation, offering long-term scalability and cost efficiency.\n3. Solution Architecture The Smart Office system adopts a fully serverless AWS architecture optimized for cost efficiency and scalability. Data from multiple sensor hubs is transmitted to AWS IoT Core, processed by Lambda functions, and stored in DynamoDB for real-time monitoring and configuration management. EventBridge automates scheduled device actions and anomaly detection, while SNS handles system notifications. The web dashboard is hosted on S3 and delivered securely via CloudFront, with user authentication managed through Amazon Cognito. This architecture minimizes operational overhead and ensures high reliability for smart environment control.\nAWS Services Used AWS IoT Core: Ingests and manages MQTT data from eight smart room hubs, enabling near real-time monitoring and automation. AWS Lambda: Handles sensor data processing, configuration updates, and automation triggers (four primary functions). Amazon API Gateway: Connects the web client to backend services through secure RESTful APIs. Amazon DynamoDB: Stores user profiles, office and room configurations, and recent sensor logs for fast access. Amazon EventBridge: Triggers scheduled automation events and anomaly detection workflows. Amazon SNS: Delivers alerts and notifications when anomalies or threshold events occur. Amazon S3: Hosts the web dashboard and static assets for the Smart Office application. Amazon CloudFront: Distributes the web dashboard globally with secure HTTPS access and low latency. Amazon Cognito: Manages authentication and authorization for users interacting with the system. Amazon CloudWatch: Monitors metrics and stores operational logs for debugging and performance optimization. Component Design Sensor Hubs: Each IoT-enabled room device collects environmental data (temperature, humidity, light, etc.) and sends it to AWS IoT Core every two minutes. Data Ingestion: AWS IoT Core routes incoming MQTT messages to the SensorProcessor Lambda, which validates and logs the data into Amazon DynamoDB. Configuration Management: The RoomConfigHandler Lambda updates room settings (auto/manual modes, thresholds, timers) in DynamoDB when users make changes through the dashboard. Automation Control: AutomationSetup Lambda listens to DynamoDB Streams for configuration updates and registers automation events in Amazon EventBridge. Event Handling: EventBridge triggers AutomationHandler Lambda at scheduled times or when anomalies are detected, sending alerts via Amazon SNS if needed. User Interaction: The web dashboard (hosted on Amazon S3 and delivered via CloudFront) allows users to view sensor data and manage configurations. User Authentication: Amazon Cognito secures user access with sign-up, sign-in, and token-based authorization integrated through API Gateway. Monitoring \u0026amp; Reliability: Amazon CloudWatch collects logs and metrics from all Lambda functions, with SQS queues as DLQs to capture failed executions for debugging. 4. Technical Implementation Implementation Phases\nBuild Theory and Research AWS Services: Study core AWS components (IoT Core, Lambda, S3, API Gateway, EventBridge, CloudFront, Cognito) and learn their integration for IoT data handling and automation (7 weeks). Design Architecture and Estimate Costs: Create the serverless architecture diagram for an 8-room smart office and use the AWS Pricing Calculator to forecast monthly expenses (1 week). Develop and Optimize Solution: Implement scripts that simulate IoT sensor data to send to AWS IoT Core, build Lambda functions for automation and anomaly detection, and connect the web dashboard with API Gateway and DynamoDB (3 weeks). Testing and Deployment: Deploy all AWS services, test system reliability and rule triggers, validate message flows from IoT Core through EventBridge and Lambda, and monitor real-time results on the hosted dashboard (1 week). Technical Requirements\nSensor Hub Network: Each room includes a Sensor Hub with temperature, humidity, and light sensors, controlled by an ESP32 or similar microcontroller. Hubs send sensor data every 2 minutes via MQTT to AWS IoT Core when in auto mode. Smart Office Platform: Implements a serverless AWS architecture using AWS IoT Core (data ingestion and rule engine), AWS Lambda (data processing and automation logic), Amazon DynamoDB (device configuration and user data), Amazon S3 (web hosting), Amazon API Gateway (REST API), Amazon EventBridge (scheduled automation and anomaly detection), Amazon SNS (notifications), Amazon CloudFront (content delivery), and Amazon Cognito (user authentication). Deployment and Tools: All services are provisioned via AWS CDK/SDK. The web dashboard, built with Next.js and hosted on S3 + CloudFront, allows configuration, real-time monitoring, and automation scheduling while minimizing backend Lambda execution time. 5. Timeline \u0026amp; Milestones Project Timeline\nWeeks 1–7: Study and research AWS services including IoT Core, Lambda, DynamoDB, S3, API Gateway, and Cognito. Week 8: Design the full architecture and estimate costs using the AWS Pricing Calculator. Weeks 9–11: Develop and integrate all system components — data simulation scripts, Lambda functions, database setup, and web dashboard. Week 12: Test, debug, and deploy the system to production. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nOr you can download the Budget Estimation File.\nInfrastructure Costs AWS Services:\nAmazon DynamoDB: Free (0.00208 GB/month) AWS Lambda: Free (119,000 requests/month, 13,386.25 GB/s) AWS IoT Core: $0.18/month (8 devices, 13,000 messages/month) AWS API Gateway: Free (720 requests/month) Amazon Simple Storage Service (S3): Free (0.01 GB) Amazon CloudFront: $1.27/month (10,000 requests/month) Amazon EventBridge: Free (2600 events/month) Amazon Simple Queue Service (SQS): Free (2600 requests/month) Amazon CloudWatch: $0.25/month (0,3612736 GB/month) (set retention 3 days) Amazon Amazon Simple Notification Service (SNS): $0.02/month (2100 requests/month, 2100 emails/month) Amazon Cognito: Free (3 MAU/month) Hardware: Existing smart office sensors and hubs — no additional cost. Total: ≈ $1.81/month, or $21.72/year (within AWS Free Tier limits). 7. Risk Assessment Risk Matrix IoT Connectivity Issues: Medium impact, medium probability. Sensor Hub Malfunction: High impact, low probability. Unexpected AWS Charges: Medium impact, low probability. Data Inconsistency or Delay: Medium impact, medium probability. Mitigation Strategies Connectivity: Implement message buffering at Sensor Hub; retry on reconnect. Hardware: Keep spare hubs and perform periodic health checks. Cost: Use AWS Budgets and Cost Explorer to monitor spending within Free Tier. Data Reliability: Validate incoming IoT data via Lambda before storing in DynamoDB. Contingency Plans Switch to manual operation if IoT service disruption occurs. Enable CloudWatch alerts for early issue detection. Use AWS CloudFormation or CDK rollback for quick recovery and cost control. 8. Expected Outcomes Technical Improvements: Real-time monitoring and automation replace manual room control. Centralized platform enhances data accuracy and consistency. Scalable architecture supports future expansion to more rooms or other IoT devices. Long-term Value: Provides a reusable foundation for smart building and IoT automation research. Enables data-driven insights for energy efficiency and environmental optimization. Demonstrates a fully serverless, low-cost AWS architecture (under $2/month). "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.3-set-up-amazon-dynamodb/5.3.3-add-items/",
	"title": "Add item to tables",
	"tags": [],
	"description": "",
	"content": " In tab Tables, choose SmartOffice_User_Prod Choose Action, Create item Choose JSON view, turn off View DynamoDB JSON Enter { \u0026#34;userId\u0026#34;: \u0026#34;user-001\u0026#34;, \u0026#34;companyId\u0026#34;: \u0026#34;comp-abc\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van A\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;a@abc.com\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;offices\u0026#34;: [ \u0026#34;office-hcm\u0026#34;, \u0026#34;office-hn\u0026#34; ] } Click Create item Do the same for table: SmartOffice_Office_Prod { \u0026#34;officeId\u0026#34;: \u0026#34;office-hcm\u0026#34;, \u0026#34;companyId\u0026#34;: \u0026#34;comp-abc\u0026#34;, \u0026#34;officeName\u0026#34;: \u0026#34;Headquarters HCM\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;Ho Chi Minh City\u0026#34;, \u0026#34;latitude\u0026#34;: 10.762622, \u0026#34;longitude\u0026#34;: 106.660172, \u0026#34;timezone\u0026#34;: \u0026#34;Asia/Ho_Chi_Minh\u0026#34;, \u0026#34;rooms\u0026#34;: [\u0026#34;A101\u0026#34;, \u0026#34;A102\u0026#34;, \u0026#34;A103\u0026#34;, \u0026#34;A104\u0026#34;] } SmartOffice_RoomConfig_Prod { \u0026#34;roomId\u0026#34;: \u0026#34;A101\u0026#34;, \u0026#34;officeId\u0026#34;: \u0026#34;office-hcm\u0026#34;, \u0026#34;temperatureMode\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;humidityMode\u0026#34;: \u0026#34;manual\u0026#34;, \u0026#34;lightMode\u0026#34;: \u0026#34;off\u0026#34;, \u0026#34;targetTemperature\u0026#34;: 26, \u0026#34;targetHumidity\u0026#34;: 60, \u0026#34;targetLight\u0026#34;: 300, \u0026#34;lastUpdate\u0026#34;: \u0026#34;2025-10-16T14:05:00Z\u0026#34;, \u0026#34;autoOnTime\u0026#34;: \u0026#34;18:00\u0026#34;, \u0026#34;autoOffTime\u0026#34;: \u0026#34;22:00\u0026#34; } SmartOffice_RoomLog_Prod { \u0026#34;roomId\u0026#34;: \u0026#34;A101\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;1734355200\u0026#34;, \u0026#34;temperature\u0026#34;: 30, \u0026#34;humidity\u0026#34;: 70, \u0026#34;light\u0026#34;: 400, \u0026#34;expireAt\u0026#34;: 1734528000 } "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.4-set-up-amazon-cognito/5.4.3-create-user/",
	"title": "Create user",
	"tags": [],
	"description": "",
	"content": " In tab User pools, choose the pool you have created Navigate to tab User, click Create user For Invitation message, choose Don\u0026rsquo;t send an invitation For Email address, enter your email for user Check Mark email address as verified For Temporary password, choose Set a password For Password, enter your initial password for user (e.g. TempPassword123!) Click Create user "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.3-set-up-amazon-dynamodb/",
	"title": "Set up Amazon DynamoDB",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will configure Amazon DynamoDB tables to store and manage Smart Office data. Each table supports specific functions such as room configuration, user automation schedules, and real-time logs. You will also enable Time to Live (TTL) for automatic cleanup, Point-in-Time Recovery (PITR) for data protection, and DynamoDB Streams to trigger AWS Lambda functions upon data changes.\nWhy using DynamoDB with TTL, PITR, and Streams: - TTL (Time to Live) automatically removes expired items (e.g., old logs), optimizing storage and cost. - PITR (Point-in-Time Recovery) ensures data resilience, allowing full table recovery within 35 days. - Streams capture real-time updates (both old and new images) and trigger Lambda to sync room modes or automation changes with IoT Core instantly.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Jumpstart your cloud career with AWS SimuLearn The blog post “Jumpstart your cloud career with AWS SimuLearn” introduces AWS SimuLearn, an interactive learning platform that combines Generative AI with realistic customer simulations. It allows learners to build both technical and communication skills in a safe, feedback-driven environment, with training tailored by role or industry. Through the stories of Hetvi, Karishma, and Kattie, the post highlights how SimuLearn helps users strengthen customer engagement skills, deepen technical expertise, and gain industry-specific knowledge in areas like healthcare. With over 200 training modules and simulations, AWS SimuLearn empowers early-career cloud professionals to grow their AWS knowledge, boost confidence, and accelerate their cloud career readiness.\nBlog 2 - How to Set Up Automated Alerts for Newly Purchased AWS Savings Plans The blog post “How to Set Up Automated Alerts for Newly Purchased AWS Savings Plans” explains how to implement an automated monitoring and alerting system for newly purchased AWS Savings Plans with low utilization. The solution leverages AWS CloudFormation to create Step Functions, SNS topics, EventBridge schedulers, and necessary IAM roles. The deployment is split into two parts:\nMember Account: hosts a Step Function that checks Savings Plans purchased within the last 7 days and the current month, sending email alerts if utilization is below a defined threshold. Management Account: provides IAM roles and access so the Step Function can analyze Savings Plans across the organization. Users can configure thresholds, scheduling frequency, and alert recipients. Upon receiving alerts, FinOps teams should review utilization data and consider returning underused Savings Plans if eligible. This solution helps FinOps teams proactively manage cloud costs, detect underutilized commitments, and improve overall Savings Plans utilization efficiency across AWS environments. Blog 3 - Operating BYOL Windows Server Workloads Effectively on AWS This AWS blog explains how to effectively operate Bring Your Own License (BYOL) Windows Server workloads on AWS to reduce costs. It covers eligibility requirements for BYOL licenses, preparing Windows Server images using VM Import/Export and Migration Hub Orchestrator, and managing license type conversions with AWS License Manager. The post also shows how to detect configuration issues using AWS Config custom rules and analyze cost data with AWS Cost and Usage Reports (CUR). By following these best practices, organizations can optimize Windows Server costs, ensure license compliance, and improve operational efficiency on AWS.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.4-set-up-amazon-cognito/5.4.4-add-user-to-database/",
	"title": "Add user to database",
	"tags": [],
	"description": "",
	"content": " In tab Users, choose the User you have created Copy User ID (Sub) and email Open Management Console, search and choose DynamoDB Navigate to tab Tables, choose SmartOffice_User_Prod Choose Action, Create item Choose JSON view, turn off View DynamoDB JSON Enter this JSON with copied userId and email { \u0026#34;userId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;companyId\u0026#34;: \u0026#34;comp-abc\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van B\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;offices\u0026#34;: [ \u0026#34;office-hcm\u0026#34; ] } Click Create item "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00, October 03, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: Introduction to how AI tools such as Amazon Q Developer and Kiro transform the software development process. Focus on two key methodologies — AI-Driven Development Lifecycle (AI-DLC) emphasizing automation, speed, and scalability, and Spec-Driven Development (SDD) with Kiro emphasizing precision and structure. Presentation of the evolving role of AI in IT, where AI assists or manages development tasks while human validation remains essential.\nOutcome: Better understanding of integrating AI into software engineering while maintaining human-centered validation. Clear insight into the strengths and limitations of AI-DLC and SDD. Exposure to tools like Amazon Q Developer and Kiro, with practical applications for improving efficiency, quality, and innovation in real-world projects.\nEvent 2 Event Name:\nDate \u0026amp; Time:\nLocation:\nRole: Attendee\nDescription:\nOutcome:\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.4-set-up-amazon-cognito/",
	"title": "Set up Amazon Cognito",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will configure Amazon Cognito User Pool to manage and authenticate all users for the Smart Office. This service validates user credentials (email/password) via the AuthenticateHandler Lambda and issues JSON Web Tokens (JWT) upon successful login. You will also configure this User Pool as an API Gateway Authorizer to secure all protected API endpoints.\nWhy using this Cognito Architecture: - ADMIN_NO_SRP_AUTH Flow centralizes authentication logic within the AuthenticateHandler Lambda, enhancing security by keeping sensitive app client logic and credentials off the frontend. - JWT (JSON Web Tokens) provide a stateless, secure \u0026ldquo;passport\u0026rdquo; that the frontend uses to prove identity for all subsequent API calls. - API Gateway Authorizer automatically validates the JWT on every request, blocking unauthorized access to protected APIs (like controlling devices) before they can execute.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.5-set-up-role-for-aws-lambda-function/",
	"title": "Set up IAM Role for AWS Lambda Functions",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Secure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.6-set-up-aws-lambda-function/",
	"title": "Set up AWS Lambda Functions",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]