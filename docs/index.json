[
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Jumpstart your cloud career with AWS SimuLearn by Denee McCloud, Hetvi Parsana, Karishma Damania, and Kattie Sepehri | on 25 JUN 2025 | in Amazon API Gateway, Amazon Bedrock, Amazon DynamoDB, Amazon SageMaker, AWS Config, AWS Lambda, Best Practices, Generative AI, Healthcare | Permalink | Share\nFor early-career cloud professionals, gaining hands-on experience with real customer engagements can be challenging. Even when shadowing opportunities exist, the fast-paced nature of these interactions rarely allows time to pause, reflect, and truly understand complex concepts.\nAWS SimuLearn addresses this gap by combining generative AI-driven customer simulations with practical technical training. Powered by Amazon Bedrock, AWS SimuLearn provides an immersive, risk-free environment where you can develop both technical and soft skills through interactive customer conversations, solution concept videos, hands-on labs, and practical exercises—all at your own pace.\nIn this post, we’ll follow three early-career AWS Solutions Architects who used these role-based learning plans and simulations to enhance their cloud expertise and customer-facing readiness. Their experiences demonstrate how structured practice in a controlled environment can accelerate professional development.\nHetvi’s story: Role-based learning focused on generative AI As a new Solutions Architect, I faced a common challenge: how to effectively communicate complex technical solutions to business leaders. While I was confident in my technical knowledge, translating cloud concepts into business value proved to be a different skill altogether.\nAWS SimuLearn transformed my approach to customer conversations. Through interactive simulations, I practiced engaging with virtual customers who needed generative AI solutions for their business challenges.\nOne memorable scenario involved helping a retail client understand how AI could enhance their customer experience through personalized shopping recommendations. I was provided with immediate feedback on both my technical recommendations and communication style. I learned to replace technical jargon with business outcomes, focusing on ROI and operational improvements rather than architectural specifications.\nWhat made the experience invaluable was the ability to retry conversations and experiment with different approaches. Each iteration helped me refine my message and build confidence in handling complex customer interactions.\nToday, I can confidently bridge the gap between technical solutions and business value, a skill that’s proven essential in my role as a Solutions Architect. For anyone looking to enhance their customer communication skills in technical roles, AWS SimuLearn offers a risk-free environment to practice and grow.\nKarishma’s story: Building technical depth As a Solutions Architect, I’ve found that deep technical expertise is essential for designing effective customer solutions. To expand my technical depth, I’ve combined individual training topics in AWS SimuLearn to create a structured learning experience tailored to real-world technical challenges.\nI started with security at the edge, progressing through modules on network security, encryption, identity and access management, and finally serverless technologies like Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. Each module included simulated customer interactions where I practiced gathering requirements and proposing solutions, followed by hands-on labs to solidify my understanding.\nBy combining training modules, I was able to simulate customer-specific workflows, ensuring a deeper grasp of their challenges. This personalized approach didn’t just prepare me for customer presentations, it gave me a framework to continuously refine my technical depth. With AWS SimuLearn, I can now rapidly upskill in unfamiliar areas and confidently architect solutions that align with business objectives, making me a more effective Solutions Architect.\nKattie’s story: Industry-based learning My background is in designing and developing software tools for Health Care and Life Sciences (HCLS) research teams. While I’m familiar with several programming languages, I had limited cloud knowledge at the level of an AWS Certified Solutions Architect – Associate and wanted to expand my knowledge about how AWS can be utilized in the HCLS industry. Additionally, I wanted to gain in-depth knowledge about various AWS services, use cases, and integration with third-party software.\nThe AWS SimuLearn: Healthcare Learning Plan provided a wide range of business problems that gave me the opportunity to experiment with various AWS services to find technical solutions for the business problems. For example, I experimented with services relevant to HIPAA compliance with AWS Config and AWS Systems Manager and AI with Amazon SageMaker, as well as batch processing, analytics, database and storage, dashboards with real-time data processing, IoT, and more.\nEach learning plan can be started using the Open Dialogue mode or the Scripted mode. Initially, I chose the Scripted mode because it was less challenging, but once I observed a few different simulated conversations, I felt comfortable using the Open Dialogue mode. Open Dialogue mode offers an opportunity to hone your soft skills through an interactive, real-time customer conversation.\nAlong the way, AWS SimuLearn provides helpful hints to guide the conversation, as well as an incomplete architecture diagram for you to expand upon during the dialogue with the customer. You also get asked questions regarding different services to further enhance the learning experience.\nGet started with AWS SimuLearn Through simulated customer conversations and hands-on learning, AWS SimuLearn helped these three early-career professionals on their learning journeys. They received real-time feedback to improve their soft and technical skills while gaining experience in a live console environment.\nWith 200+ trainings, including learning options available by cloud role or industry, AWS SimuLearn offers you a personalized learning experience. Game-based training with simulations empowers early-career professionals to build both technical expertise and customer engagement skills in a risk-free environment.\nReady to launch your cloud career? Begin with the free AWS SimuLearn: Cloud Practitioner Learning Plan and explore the full library of topic-based simulations.\nTo learn more about what’s new with AWS SimuLearn, check out this recent blog post: Introducing AWS SimuLearn: Generative AI Practitioner.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "How to Set Up Automated Alerts for Newly Purchased AWS Savings Plans by Syed Muhammad Tawha and Dan Johns | on 26 JUN 2025 | in Amazon Simple Notification Service (SNS), AWS Cloud Financial Management, AWS CloudFormation, Cloud Cost Optimization | Permalink | Share\nAs organizations expand, FinOps teams require a comprehensive overview of AWS Savings Plans commitments to maximize utilization efficiency. This solution involves implementing monitoring systems and automated alerts to identify underutilized Savings Plans within the eligible return period.\nWhen you purchase a Savings Plan, you make a commitment for one or three years. Savings Plans with an hourly commitment of $100 or less can be returned if they were purchased within the last seven days and in the same calendar month, provided you haven’t reached your return limit. Once the calendar month ends (UTC time), these purchased Savings Plans cannot be returned.\nIn this blog post, we provide AWS CloudFormation templates that create AWS Step Functions state machine, Amazon Simple Notification Service (SNS) topic, Amazon EventBridge scheduler, and necessary AWS Identity and Access Management (IAM) roles to automate the monitoring of newly purchased Savings Plans and highlight those that are underutilized.\nOverview of Solution: This solution follows AWS security best practices by separating the deployment across two accounts. One CloudFormation stack will be created in the Management account to establish necessary IAM roles for fetching Savings Plans utilization. Another CloudFormation stack will be deployed in your chosen Member Account within your AWS Organization.\nThe CloudFormation stack in a member account creates a state machine that assumes a role in your management account and analyzes all Savings Plans in your management account, including those purchased across your organization. The workflow filters active Savings Plans based on their purchase date, focusing specifically on plans acquired within the last 7 days and the current calendar month. It then evaluates their utilization rates and identifies plans falling below the defined threshold.\nThe state machine executes at your specified frequency and uses Amazon SNS to send email alerts to addresses you provide during CloudFormation stack creation. These alerts contain detailed information about low-utilization Savings Plans and instructions for the return process.\nFigure 1: AWS architecture diagram – Member account assumes a role to read Savings Plans data from the management account and triggers a Step Function, which sends email alerts via SNS. Solution Walk Through: Prerequisites An AWS account IAM permissions to create a CloudFormation Stack and deploy an IAM role in the management Account IAM permissions to create a CloudFormation Stack and deploy Step Functions, IAM roles, SNS, and EventBridge scheduler in your chosen member Account Deploy the solution In this section we will deploy resources for this solution in your accounts:\nPart 1 – Member Account Deployment In this section, we will deploy resources for this solution in your chosen member account.\nLogin to your AWS Management Console of the member account where you want this solution to run Deploy this CloudFormation Stack Provide the Stack Name as new-sp-utilization-alert-member In the AlertEmails parameter, enter a comma-separated list of email addresses that will receive notifications about underutilized Savings Plans. In the ManagementAccountId parameter, enter the 12 digit AWS Account Id of your AWS management account. In the ScheduleExpression parameter, specify the execution frequency for the Step Functions state machine using cron format (default is daily at 9 AM UTC). In the UtilizationThreshold parameter, specify the minimum utilization percentage for your Savings Plans. You receive alerts when utilization falls below this threshold. Click Next, select the acknowledgment box, and create the stack Wait until the stack has finished deploying and is showing as CREATE-COMPLETE You will receive an email to confirm your subscription to the SNS topic created by this stack. Please confirm the subscription to begin receiving notifications. Visit the Outputs tab of the stack you just created and make a note of the values of the ExecutionRoleArn and StateMachineArn Keys, you will need these in the next part. Part 2 – Management Account Deployment Log in to your AWS Management Console. Note: This must be the same account as the one entered in the ManagementAccountId parameter in the previous part. Deploy this CloudFormation stack Provide the Stack Name as new-sp-utilization-alert-management In the ExecutionRoleArn parameter, provide the value copied from the stack outputs of the stack deployed in the member account. In the StateMachineArn parameter, provide the value copied from the stack outputs of the stack deployed in the member account. Click Next, select the acknowledgment box, and create the stack Wait until the stack has finished deploying and is showing as CREATE-COMPLETE Test the Solution Now that the Step Functions state machine and associated resources are deployed in your member account, let’s test the deployment:\nLogin back in to your AWS Management Console of the member account where you deployed part 1 of this solution. Navigate to the Resources tab in your CloudFormation stack and locate the SavingsPlansAlerts Step Functions state machine. Click the blue hyperlink. You will be redirected to the Step Functions console. Click the Start execution button on the right. View the execution details in the Events section to monitor the state machine’s progress. If you have any Savings Plans purchased within the last 7 days and the current calendar month, you will receive email notifications. A successful execution is indicated by a green box in the Graph view. If any Savings Plans fall below your specified utilization threshold, you will receive an email at your provided address. Clean Up All resources deployed for this solution can be removed by deleting the CloudFormation stacks. You can delete the stack through either the AWS Management Console or the AWS CLI.\nTo delete the management account stack (CLI):\naws cloudformation delete-stack –stack-name new-sp-utilization-alert_management To delete the member account stack (CLI):\naws cloudformation delete-stack –stack-name new-sp-utilization-alert_member Understanding Alerts and Taking Action When you receive an alert about underutilized Savings Plans, you should review the utilization details provided in the email notification. Analyze your utilization metrics against the original commitment you made when purchasing the Savings Plan, and investigate whether the low utilization is an expected or due to other factors such as workload migration, architectural changes, or miscalculated capacity needs. Consider returning the Savings Plan if the utilization remains consistently below your threshold, the plan was purchased within the last 7 days, the purchase occurred in the current calendar month, and the hourly commitment is $100 or less. Document the return reason for future reference and planning.\nConclusion In this post, we explored how to use the Savings Plan and Cost Explorer APIs to identify underutilized Savings Plans in your organization. We then demonstrated how to use a Step Functions State Machine to filter Savings Plans purchased within the last 7 days and the current calendar month. This timing is crucial because you can return Savings Plans within the return window if they were purchased inadvertently or aren’t being utilized effectively. For guidance on returning a purchased Savings Plan, please refer to the Returning a Purchased Savings Plan documentation.\nSyed Muhammad Tawha\nSyed Muhammad Tawha is a Principal Technical Account Manager at AWS based in Dublin, Ireland. Tawha specializes in Storage, Resilience and Cloud Cost Optimization. He is passionate about helping AWS customers. Tawha also loves spending time with his friends and family. Dan Johns\nDan Johns is a Senior Solutions Architect Engineer, supporting his customers to build on AWS and deliver on business requirements. Away from professional life, he loves reading, spending time with his family and automating tasks within their home. "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Operating BYOL Windows Server Workloads Effectively on AWS by Ali Alzand, Jon Madison, and Mike Gupta | on 01 JUL 2025 | in Amazon EC2, AWS Config, AWS Cost and Usage Report, AWS License Manager, AWS Migration Hub, Technical How-to, Windows on AWS | Permalink | Share\nOne way that customers running Microsoft Workloads on Amazon Web Services (AWS) may reduce costs is taking advantage of Bring Your Own License (BYOL) for eligible licenses they own. In this blog post, we are going to share a few practices to help you optimize your operation of BYOL Windows Server workloads on AWS.\nIntroduction A common way to run your Windows Server workloads on Amazon Elastic Compute Cloud (Amazon EC2) is to use the “license included” option. This has the benefit of not having to purchase or manage your own licenses and the flexibility of per-second billing. However, if you have already purchased licenses and they are eligible for use on AWS, then it makes sense to bring them and reduce your costs accordingly.\nWe will review several specific techniques to help you when running BYOL Windows Server workloads on AWS. They are:\nPreparing your on-premises servers for import to AWS as Amazon Machine Images (AMIs). Transition and manage your Windows licenses from BYOL to license included when appropriate. Detecting configuration issues using an AWS Config custom rule. Understanding data related to your BYOL Windows instances in the AWS Cost and Usage Report (AWS CUR). BYOL for Windows on AWS To take advantage of BYOL, you need to confirm that they are eligible. AWS provides guidance for Microsoft Licensing on AWS. When determining if your Windows licenses are eligible for BYOL on AWS, consider:\nLicenses must be perpetual, and purchased before October 1, 2019, or as a true-up on an Enterprise Agreement (EA) that was active at that time. The Windows version must be Windows Server 2019 or earlier. If your licenses are eligible, then you can use them on AWS. Regardless of whether or not you have Software Assurance on your licenses, Windows Server is not eligible for License Mobility. This means that the licenses will need to apply to hardware dedicated to you alone. Amazon EC2 Dedicated Hosts are a solution that fulfills this requirement. Dedicated Hosts provide you with a familiar experience for running your Amazon EC2 instances, without the need to manage hardware or a hypervisor. AWS License Manager is a service used to manage licenses in AWS, and it is key to an effective BYOL Windows strategy.\nThe billing for your Amazon EC2 Windows instances is determined from the usage operation field that the instance inherits from its source AMI. Windows instances that run with the license included, regardless of tenancy, use the usage operation of RunInstances:0002. However, when you use your own license for a Windows instance on dedicated hosts, the usage operation of RunInstances:0800 is required. The how to create an Amazon EC2 AMI usage and billing information report blog post will help you generate the usage operation for the instances in your organization.\nPreparing your images for BYOL One requirement for using your own Windows licenses on AWS is to supply your own AMI, rather than using one created by AWS. When bringing your own image to AWS, you have different options to produce them. If the destination for your Windows server is BYOL on dedicated hosts, these tools will help you ensure your AMI is ready for use.\nVM Import/Export (VMIE) is a tool that helps you to import virtual machine images from your existing virtualization platform as Amazon Machine Images. The first step is to export your virtual machine using a standard format such as Open Virtual Appliance (OVA), ESX Virtual Machine Disk (VMDK), or Virtual Hard Disk (VHD/VHDX). Then, upload the image to an Amazon Simple Storage Service (S3) bucket in anticipation of the conversion process.\nTo use VMIE, use these instructions to create an AWS Identity and Access Management (IAM) role named “vmimport” that the service will use to perform operations on your behalf.\nWhen using the AWS Command Line Interface (AWS CLI) to import a Windows image that you are planning on using for BYOL on dedicated hosts, it is necessary to specify the license type to set the usage operation correctly on the resultant AMI. To import an image, a command such as the following can be used (in this case for an OVA image in an S3 bucket):\naws ec2 import-image –usage-operation RunInstances:0800 –disk-containers Format=OVA,Url=s3://\u0026lt;\u0026lt;my-bucket\u0026gt;\u0026gt;/\u0026lt;\u0026lt;my-image-name\u0026gt;\u0026gt;.ova This will start an import job that, once completed, will yield an AMI with the proper usage code for Windows BYOL.\nMigration Hub Orchestrator is a tool that lets you create workflows to automate tasks and simplify the migration process. One of the workflow templates that Orchestrator provides is “Import virtual machine images to AWS”. Use this workflow to import an image for Windows BYOL.\nOpen the AWS console and navigate to the Migration Hub Console. Choose Workflows in the Orchestrate side menu. Choose Create Workflow (Figure 1) Figure 1: Create Workflow Select the Import virtual machine images to AWS template (Figure 2) and choose Next Figure 2: Select the import virtual machine template On the Configure your workflow page, enter a Name for the workflow, and optionally enter a Description. In the Source environment configuration section, populate the Disk container field, which is the S3 bucket where you stored your image from on premises. The name must conform to the requirements from the Migration Hub Orchestrator documentation. Figure 3: Configure source environment In the Target environment configuration section, select the operating system and license for the virtual machines created with the resultant AMI. Choose Windows Server BYOL without SQL Server. Figure 4: Choose the licensing model Use the rest of the fields to further customize your AMI based on your requirements. These include the boot mode, AWS Key Management Service (KMS) encryption key, tags and license specification (for business case analysis). You also have the option to leave these with their default values. Choose On the Review and submit page, choose After uploading an image and creating your workflow, it is ready to run by choosing Run workflow.\nFigure 5: Run Workflow Managing license conversion properly There are scenarios in which you will need to switch Amazon EC2 instances from the BYOL licensing model to license included and vice versa. These include (but are not limited to):\nUpgrading the operating system of the Amazon EC2 instance to Windows Server 2022, which is not eligible for BYOL, regardless of tenancy. Moving an Amazon EC2 instance off a dedicated host to run it on shared tenancy EC2, which is not eligible for BYOL. Moving an Amazon EC2 instance that is eligible for BYOL from shared tenancy to a dedicated host. When you need to switch the licensing model of an Amazon EC2 instance, use the License type conversion feature in AWS License Manager. License type conversion lets you change the usage operation. See our guide for eligible license types for Windows and SQL Server in License Manager.\nDetecting configuration issues with AWS Config AWS Config is a service that helps you assess, audit, and evaluate the configuration of your AWS resources. By leveraging a custom AWS Config Rule, you can detect potential license misconfiguration in instances running on dedicated hosts, saving unnecessary licensing costs.\nThe aws-config-rules repository contains custom AWS Config Rules to deploy to your AWS account using the AWS Config Rules Development Kit (RDK). Use the custom AWS Config Rule called EC2_INSTANCE_LICENSE_INCLUDED_DEDICATED_HOST to detect instances with license-included Windows Server (usage operation RunInstances:0002) running on Dedicated Hosts.\nUse AWS CloudShell to run the RDK and test the AWS Config rules deployment. To install the custom rule, open CloudShell in the AWS Console in the desired AWS Region, and run the following commands:\npip install rdk rdk init git clone https://github.com/awslabs/aws-config-rules cd aws-config-rules/python rdk deploy EC2_INSTANCE_LICENSE_INCLUDED_DEDICATED_HOST Once the rule has completed deployment, view the rule in the AWS Config console. For instances with mis-configured licenses, either move them to Shared tenancy or follow the License Conversion process accordingly.\nFigure 6: Custom Config rule Understanding CUR data for BYOL instances AWS Cost and Usage Reports (CUR) contains the most comprehensive set of cost and usage data available. Use Amazon Athena to query your CUR data. The following query shows the licenses your instances are being billed for:\nselect line_item_resource_id, line_item_operation, line_item_line_item_type, month, year, line_item_unblended_cost, line_item_blended_cost, line_item_usage_type, line_item_usage_account_id, line_item_line_item_description from customer_all where line_item_usage_account_id = \u0026#39;[ACCOUNT NUMBER]\u0026#39; and line_item_line_item_type = \u0026#39;Usage\u0026#39; and line_item_operation like \u0026#39;%RunInstances:%\u0026#39; Based on the results of the above Query, the line_item_operation field shows what you’re being billed for.\nFigure 7: AWS CUR output Conclusion Implementing BYOL for Windows Server workloads on AWS successfully, requires careful attention to license eligibility, configuration, and ongoing management. By understanding the key requirements – from license purchase dates and Windows Server versions to proper usage operation codes on dedicated hosts – organizations can effectively reduce their cloud infrastructure costs while maintaining compliance. Success depends on three key elements:\nProper license evaluation – identifying eligible licenses based on purchase date and Windows Server version Accurate configuration – ensuring correct usage operation codes to avoid double-billing on dedicated hosts Ongoing monitoring – maintaining regular assessment of usage and costs By following these practices, organizations can optimize their Windows Server deployment costs while maintaining licensing compliance on AWS.\nReady to start optimizing your Windows Server costs on AWS? Request an AWS Optimization and Licensing Assessment to begin evaluating your licensing opportunities and potential cost savings.\nAWS has significantly more services, and more features within those services, than any other cloud provider, making it faster, easier, and more cost effective to move your existing applications to the cloud and build nearly anything you can imagine. Give your Microsoft applications the infrastructure they need to drive the business outcomes you want. Visit our .NET on AWS and AWS Database blogs for additional guidance and options for your Microsoft workloads. Contact us to start your migration and modernization journey today.\nTAGS: Amazon EC2, AWS License Manager, Cost Savings, Microsoft, Windows On AWS, Windows Server\nAli Alzand\nAli is a Microsoft Specialist Solutions Architect at Amazon Web Services who helps global customers unlock the power of the cloud by migrating, modernizing, and optimizing their Microsoft workloads. He specializes in cloud operations — leveraging AWS services like Systems Manager, Amazon EC2 Windows, and EC2 Image Builder to drive cloud transformation. Outside of work, Ali enjoys exploring the outdoors, firing up the grill on weekends for barbecue with friends, and sampling all the eclectic food has to offer. Jon Madison\nJon Madison is a Pr. Delivery Consultant on the AWS Professional Services (ProServe) Energy Team. He has a background in Cloud Infrastructure, Security, and DevOps, and is passionate about helping customers with cloud adoption and building scalable solutions and processes. In his free time Jon enjoys cooking, gaming, and spending time with his family and friends. Mike Gupta\nMike Gupta is a Senior Technical Account Manager at AWS based out of New York City. In his role, he provides strategic technical guidance to help customers use AWS best practices to plan and build solutions. He’s dedicated to empower customers to develop scalable, resilient, and cost-effective architectures. In his free time, Mike enjoys spending time with his wife and family, exploring local history and trying new restaurants. "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.3-amazon-dynamodb/5.3.1-create-tables/",
	"title": "Create tables",
	"tags": [],
	"description": "",
	"content": " Open Management Console, search and choose DynamoDB. Navigate to tab Tables, click Create table For Table name, enter SmartOffice_RoomLog_Prod For Partition key, enter roomId For Sort Key, enter timestamp For Table settings, choose Customize settings In the field Table class, choose DynamoDB Standard-IA Log sensor with less access, need to optimize storage pricing.\nLeave other setting at default Add Tag for management (Key: Project, Value: SmartOffice; Key: Environment, Value: Dev) Click Create table Same for table: SmartOffice_User_Prod Name: SmartOffice_User_Prod Partition key: userId Table settings: Customize settings Deletion protection: Check Turn on deletion protection Tag : Key: Project, Value: SmartOffice; Key: Environment, Value: Dev SmartOffice_Office_Prod Name: SmartOffice_Office_Prod Partition key: officeId Table settings: Customize settings Deletion protection: Check Turn on deletion protection Tag : Key: Project, Value: SmartOffice; Key: Environment, Value: Dev SmartOffice_RoomConfig_Prod Name: SmartOffice_RoomConfig_Prod Partition key: roomId Table settings: Customize settings Deletion protection: Check Turn on deletion protection Tag : Key: Project, Value: SmartOffice; Key: Environment, Value: Dev "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: “AI-Driven Development Life Cycle: Reimagining Software Engineering” Event Objectives Explores AI-Driven Development using Amazon Q Developer Explores Spec-Driven Development using Kiro Speakers Toan Huynh – SA My Nguyen – ISV Account Director Key Highlights The Role of AI in IT 2023: AI acted as an assistant, helping developers minimize workload, recommend code, and check errors 2024: AI became embedded in IDEs, enabling system design and automated workflows 2025: Software engineers with strong AI skills can develop products in minimal time Developing Products with AI While Keeping Humans at the Center Developers can use AI to generate code but they still must validate its correctness and effectiveness The owner of AI-generated code is the developer, since they are responsible for validating it before implementation Coding with AI usually leads to two paths: AI-Assistant: Developers see AI as a co-worker who suggests code and solutions AI-Manager: AI takes full responsibility for designing systems, generating and implementing code, and maintaining the product AI-Driven Development Lifecycle (AI-DLC) AI acts as an assistant that helps to develop the requirements and specifications AI supports developers by writing code in small units step-by-step, waiting for validation before moving on to the next task It requires developers to minimize the scope and provide a specified prompt for AI Mob programming is recommended for this methodology since it reduces product development time Spec-Driven Development Lifecycle with Kiro Kiro is an IDE that supports developers by taking specifications and developing step-by-step tasks to implement them into code Kiro provides a log of all the actions and prompts, allowing developers to track progress Kiro is suitable for projects that need to have accurate specifications before starting to write code Key Takeaways Developer Mindset Human-centric: AI can design systems and write code, but developers remain responsible for validating the results Ready for changes: With powerful AI tools like Amazon Q Developer and Kiro emerging, developers should continuously prepare for new technologies Pros and Cons Using Ai-Driven or Spec-Driven Development Lifecycle depends on the scope and purpose of the project Each method has pros and cons that affect the development team as well as the product Pros AI-Driven Development Lifecycle is good for projects that need the speed, reusability, cost efficiency, and scalability Spec-Driven Development Lifecycle is good for projects that need narrative quality, consistency, and predictability Cons AI-Driven Development Lifecycle may result in inconsistent quality, ethical/IP concerns, or maintenance challenges Spec-Driven Development Lifecycle may result in slower production cycles, higher costs, or limited content Applying to Work Apply AI-DLC or SDD to current projects Learn new technologies like Amazon Q Developer or Kiro Event Experience Attending the “AI-Driven Development Life Cycle: Reimagining Software Engineering” workshop was extremely valuable. I had the opportunity to learn new methodologies and connect with IT professionals. Key experiences included:\nLearning from highly skilled speakers Learning from experts how they handle errors in coding and product development Experiencing real-world use cases of the methodologies Leveraging modern tools Explored Amazon Q Developer, an AI tool that supports AI-DLC Observed a demonstration of how Kiro is used in the Spec-Driven Development Lifecycle Overall, the event not only provided technical knowledge but also introduced to new tools that can support development lifecycle\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: “GenAI-powered App-DB Modernization workshop” Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Tran Ngoc Khiet\nPhone Number: 0908171773\nEmail: tranngockhiet22062005@gmail.com\nUniversity: FPT University Ho Chi Minh Campus\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 06/09/2025 to 14/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Team up for the project, get acquainted with FCJ mentor team. Watch videos from series First Cloud Journey Bootcamp - 2025 in Youtube from the begining to the end of module 2. Learn about AWS services and how to use them. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Team up and get acquainted with team members - Read rules, workshop guide and handling of violations - Watch video How to draw AWS architecture with draw.io - Watch video Guide for AWS workshop 09/08/2025 09/08/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://policies.fcjuni.com 3 - Watch theory videos of Module 1 - Watch videos, read documents of Lab 1, Lab 7 and Lab 9 - Practice: + Create AWS Free Tier Account + Config MFA for the account + Upgrade account to paid plan 09/09/2025 09/09/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000001.awsstudygroup.com https://000007.awsstudygroup.com https://000009.awsstudygroup.com 4 - Watch theory videos of Module 2 - Watch videos, read documents of Lab 3 - Practice: + Create VPC + Create Subnet + Create Route Table + Create Internet Gateway + Create NAT Gateway + Create Security Group + Create Network ACL + Create EC2 instances + Test connection between two EC2 Instances 09/10/2025 09/10/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000003.awsstudygroup.com 5 - Watch videos, read documents of Lab 10 and Lab 19 - Practice: + Create and test a connection to a Hybrid DNS with Route 53 + Create and test a VPC Peering connection between two EC2 instances 09/11/2025 09/11/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000010.awsstudygroup.com https://000019.awsstudygroup.com 6 - Watch videos, read documents of Lab 20 - Practice: + Create a Transit Gateway + Test the connection between EC2 instances through the Transit Gateway 09/12/2025 09/12/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000020.awsstudygroup.com Week 1 Achievements: Create AWS Free Tier account, upgrade to paid plan and config MFA sucessfully. Get used to AWS Management Console and add widgets to the screen. Create and config AWS services: VPC Subnet Route Table Internet Gateway NAT Gateway Security Group Network ACL EC2 Know how to create CloudFormation from templates. Know how to connect EC2 Intances through VPC Peering Connection or Transit Gateway. Know how to create and connect to DNS with Route 53. "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Watch videos from series First Cloud Journey Bootcamp - 2025 in Youtube from Module 3 to the end of module 4. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch theory videos of Module 3 - Watch videos, read documents of Lab 13 and Lab 24 - Practice: + Create and test the operation of the Backup Plan + Create and connect Storage Gateway to on-premise environment through file share 09/15/2025 09/15/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000013.awsstudygroup.com https://000024.awsstudygroup.com 3 - Watch videos, read documents of Lab 57 - Practice: + Create and config S3 Bucket + Host a static website with S3 and through CloudFront + Config bucket versioning, move object among Bucket and replicate object among multi-region Bucket 09/16/2025 09/16/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000057.awsstudygroup.com 4 - Watch theory videos of Module 4 - Watch videos, read documents of Lab 14 - Practice: + Import a Virtual Machine to AWS + Deploy an Instance from AMI + Export a Virtual Machine from an Instance 09/17/2025 09/17/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000014.awsstudygroup.com 5 - Watch videos, read documents of Lab 25 - Practice: + Create an SSD/HDD Multi-AZ file system and config File Share + Operate some functions of File Share through Remote Desktop 09/18/2025 09/18/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000025.awsstudygroup.com 6 - Learn Coursera course Research Methods and Research Methodologies 09/19/2025 09/19/2025 https://www.coursera.org/learn/research-methods https://www.coursera.org/learn/research-methodologies Week 2 Achievements: Know how to create Backup Plan Know how to create Storage Gateway and config File Share Know how to create and config S3 Bucket Know how to host a static website with S3 and through CloudFront Practice some of S3 Bucket functions: Bucket versioning Move object Replicate object Know how to import/export VM Know how to deploy an Instance from AMI Know how to operate some functions on File Share of an SSD/HDD Multi-AZ file system through CLI on a Remote Desktop "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Watch videos from series First Cloud Journey Bootcamp - 2025 in Youtube Module 5. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch theory videos of Module 5 - Watch videos, read documents of Lab 18 and Lab 22 - Practice: + Config Security Hub + Create and test Lambda Function 09/22/2025 09/22/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000018.awsstudygroup.com https://000022.awsstudygroup.com 3 - Watch videos, read documents of Lab 27 and Lab 28 - Practice: + Create, manage Tag and Resource Group + Config Policies for Role + Test access EC2 with Tag through IAM 09/23/2025 09/23/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000027.awsstudygroup.com https://000028.awsstudygroup.com 4 - Watch videos, read documents of Lab 30 and Lab 33 - Practice: + Upload and share encrypted data with S3 Bucket + Create and query Event History with Athena through CloudTrail table 09/24/2025 08/24/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000030.awsstudygroup.com https://000033.awsstudygroup.com 5 - Watch videos, read documents of Lab 44 and Lab 48 - Practice: + Config IAM Group, User and Role + Config Access Key and use IAM Role to access 09/25/2025 08/25/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000044.awsstudygroup.com https://000048.awsstudygroup.com 6 - Learn Coursera course Being a researcher (in Information Science and Technology) và Advanced Writing 09/26/2025 08/26/2025 https://www.coursera.org/learn/being-researcher https://www.coursera.org/learn/advanced-writing?specialization=academic-english Week 3 Achievements: Know how to enable and config Security Hub Know how to create and use Lambda Function Know how to mange resource with Tag and Resource Group Know how to attach Policies to Role Know how to share encrypted data through S3 Bucket Know how to query Event History with Athena through Cloudtrail table Know how to mange User through Group, Role and Access key "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Watch videos from series First Cloud Journey Bootcamp - 2025 in Youtube Module 6. Participate in AI-Driven Development Life Cycle: Reimagining Software Engineering event. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch theory videos of Module 6 09/29/2025 09/29/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 3 - Watch videos, read documents of Lab 5 - Practice: + Create RDS Database Instance + Deploy application with NodeJS 09/30/2025 09/30/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000005.awsstudygroup.com 4 - Watch videos, read documents of Lab 35 - Practice: + Create sample data and Data Stream + Analysis and visualize the Data with Athena and QuickSight 10/01/2025 10/01/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000035.awsstudygroup.com 5 - Translate Blogs: + Jumpstart your cloud career with AWS SimuLearn + Observing Agentic AI workloads using Amazon CloudWatch + Operating BYOL Windows Server Workloads Effectively on AWS 10/02/2025 10/02/2025 https://aws.amazon.com/blogs/training-and-certification/jumpstart-your-cloud-career-with-aws-simulearn/ https://aws.amazon.com/blogs/aws-cloud-financial-management/how-to-set-up-automated-alerts-for-newly-purchased-aws-savings-plans/ https://aws.amazon.com/blogs/modernizing-with-aws/operating-byol-windows-server-workloads-effectively-on-aws/ 6 - Participate in AI-Driven Development Life Cycle: Reimagining Software Engineering event - Write event report 10/03/2025 10/03/2025 Week 4 Achievements: Know how to create RDS Database Instance and deploy application through it Know how to create Data Stream, analysis and visualize data through Athena and QuickSight Translate three blogs to Vietnamese Participate in AI-Driven Development Life Cycle: Reimagining Software Engineering event: Learn about AI-Driven Deployment Lifecycle through Amazon Q Developer Learn about Spec-Driven Deployment Lifecycle through Kiro "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Watch videos from series First Cloud Journey Bootcamp - 2025 in Youtube Module 7. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch videos, read documents of Lab 39 - Practice: + Create DyanamoDB database + Operate some query of DynamoDB in AWS Cloudshell 10/06/2025 10/06/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000039.awsstudygroup.com/vi 3 - Watch videos, read documents of Lab 40 - Practice: + Build database from S3 Bucket and Glue Crawler + Analysis cost and usage performance 10/07/2025 10/07/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000040.awsstudygroup.com/vi 4 - Watch videos, read documents of Lab 60 - Practice: + Create DyanamoDB database + Operate some DynamoDB functions on management console and AWS SDK: * Create * Write * Read * Update * Query * Create Global Secondary Index * Query Global Secondary Index 10/08/2025 10/08/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000060.awsstudygroup.com/vi 5 - Watch videos, read documents of Lab 72 - Practice: + Create Firehose, IAM Role, Glue Crawler + Transform data with notebook and Glue DataBrew + Analysis data with Athena, Kinesis Data Analytics + Visualize data with QuickSight + Run Lambda Function on data + Create and connect Redshift cluster with S3 Bucket through Glue Connection 10/09/2025 10/10/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000072.awsstudygroup.com/vi 6 - Watch videos, read documents of Lab 73 - Practice: + Upload dataset + Create visual chart and table from data + Create backup dashboard + Publish dashboard 10/10/2025 10/10/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000073.awsstudygroup.com/vi Week 5 Achievements: Know how to create DynamoDB database know how to query data using AWS CloudShell, AWS SDK or management console Know how to build database from S3 Bucket and Glue Crawler Know how to analysis cost and usage performance of Glue database using Athena Apply knowlegde of Kinesis, IAM, Glue, Athena, QuickSight, Lambda, Redshift to do a workshop Know how to create visual chart and table from data with Quicksight Know how to create and publish dashboard with QuickSight "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Synthesize knowledge about AWS to mindmap for mid-term test Brainstorm idea for workshop Draw architect diagram for workshop Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Synthesize theory knowledge of Module 1 and Module 2 - Draw mindmap for Module 1 and Module 2 10/13/2025 10/13/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://drive.google.com/file/d/1jlnUWvbaxQmmhaDT4eMMyFaZNcN3qlxt/view?usp=drive_link 3 - Synthesize theory knowledge of Module 3 and Module 4 - Draw mindmap for Module 3 and Module 4 10/14/2025 10/14/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://drive.google.com/file/d/1jlnUWvbaxQmmhaDT4eMMyFaZNcN3qlxt/view?usp=drive_link 4 - Synthesize theory knowledge of Module 5 and Module 6 - Draw mindmap for Module 5 and Module 6 10/15/2025 10/15/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://drive.google.com/file/d/1jlnUWvbaxQmmhaDT4eMMyFaZNcN3qlxt/view?usp=drive_link 5 - Brainstorm idea for workshop 10/16/2025 10/16/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing 6 - Draw architect diagram for workshop 10/17/2025 10/17/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing Week 6 Achievements: Have a mindmap to summarize knowledge of AWS Brainstorm idea for workshop Draw architect diagram of workshop "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Caculate pricing and estimate Adjust architect diagram Write proposal Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Estimate and caculate price for: emsp; - Amazon DynamoDB emsp; - AWS Lambda emsp; - AWS iot core 08/11/2025 08/11/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing 3 - Estimate and caculate price for: emsp; - Amazon API Gateway emsp; - AWS Simple Cloud Storage emsp; - Amazon CloudFront 08/12/2025 08/12/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing 4 - Estimate and caculate price for: emsp; - Amazon EventBridge emsp; - Amazon SQS emsp; - Amazon CloudWatch 08/13/2025 08/13/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing 5 - Estimate and caculate price for: emsp; - Amazon Simple Notification Service emsp; - Amazon Cognito emsp; - Create Estimate for workshop 08/14/2025 08/15/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing 6 - Adjust architect diagram and write proposal 08/15/2025 08/15/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing Week 7 Achievements: Finish pricing and estimate Finish architect diagram Finish proposal "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 8 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 9 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Team up for project, learn Module 1 and 2\nWeek 2: Learn Module 3 and 4\nWeek 3: Learn Module 5\nWeek 4: Learn Module 6 and participate in an event\nWeek 5: Learn Module 7\nWeek 6: Create a mindmap to summarize knowleadge, brainstorm idea for workshop and draw architect diagram\nWeek 7: Caculate pricing and adjust architect diagram\nWeek 8: Week 9: Week 10: Week 11: Week 12: "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.3-amazon-dynamodb/5.3.2-config-tables/",
	"title": "Config tables",
	"tags": [],
	"description": "",
	"content": "Config TTL for table SmartOffice_RoomLog_Prod In tab Tables, choose SmartOffice_RoomLog_Prod Navigate to tab Settings, in the field Time to Live (TTL) click Turn on For TTL attribute name, enter expireAt For Simulated date and time, choose Epoch time value, enter 1770000000 Click turn on TTL Config Point-in-time recovery (PITR) for table SmartOffice_User_Prod, SmartOffice_Office_Prod and SmartOffice_RoomConfig_Prod In tab Tables, choose SmartOffice_User_Prod Navigate to tab Backups, in the field Point-in-time recovery (PITR), click Edit Check Turn on point-in-time recovery In the field Backup recovery period choose 35 days Click Save changes Do the same for table SmartOffice_Office_Prod and SmartOffice_RoomConfig_Prod Config DynamoDB stream for table SmartOffice_RoomConfig_Prod In tab Tables, choose SmartOffice_RoomConfig_Prod Navigate to tab Exports and streams, in field DynamoDB stream details, click Turn on In the field View type, choose New and old images Click Turn on stream "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "IoT Weather Platform for Lab Research A Unified AWS Serverless Solution for Real-Time Weather Monitoring 1. Executive Summary The Smart Office Management Console is designed base on the idea of weekly trip to AWS office at Ho Chi Minh city. It supports office with smart device like auto light, auto scensor window, auto air conditioner by collect, transmit and process data from IoT Sensor. By utilizing AWS Serverless services, data can transmit via MQTT and process every 5 minutes to support real-time monitoring, acting base on predictive weather and saving cost from unused devicesm with restricted to athenticated members via Amazon Cognito\n2. Problem Statement What’s the Problem? Nowadays, offices require manual control for light, window, air conditioner, .etc. Most office turn on light and air conditioner thoughout there work hour (usually 8 a.m to 5 p.m), but this may not be necessary. For example, sunlight at 8 a.m may not be bright enough for worker to see clearly, but if the office is not very big, sunlight from 10 a.m to 3.pm can support light up very much. Additionally, in rainy season, some places have high humidity which can provide a cool environment similar to air conditioner environment. If we can control devices base on these condition, it will save energy for the world as well as money for organization.\nThe Solution The platform uses AWS IoT Core to ingest MQTT data, AWS Lambda and API Gateway for processing, DynamoDB for data storage, Amazon S3 for static website hosting and Amazon Cognito ensures secure access, AWS SNS to send notifications when system monitoring device or when supisious weather condition detect. Similar to Thingsboard and CoreIoT, users can register new devices and manage connections, though this platform operates on a smaller scale and is designed for private use. Key features include real-time monitoring, weather prediction, auto controlling and low operational costs.\nBenefits and Return on Investment The Smart Office system enhances automation, monitoring, and energy efficiency across office environments, providing a reliable platform for research, expansion, and real-world IoT application development. It serves as a foundational resource for lab members and developers to build advanced smart environment solutions, while offering a centralized system that reduces manual configuration, improves data reliability, and simplifies maintenance.\nMonthly operating costs are estimated at $1.81 USD, including DynamoDB ($0.09), IoT Core ($0.18), CloudFront ($1.27), CloudWatch ($0.25) and SNS ($0.02), totaling $21.72 USD per year. As IoT hardware is already deployed, there are no additional equipment expenses. The system achieves a break-even period of 6–12 months through significant time savings and reduced manual operation, offering long-term scalability and cost efficiency.\n3. Solution Architecture The Smart Office system adopts a fully serverless AWS architecture optimized for cost efficiency and scalability. Data from multiple sensor hubs is transmitted to AWS IoT Core, processed by Lambda functions, and stored in DynamoDB for real-time monitoring and configuration management. EventBridge automates scheduled device actions and anomaly detection, while SNS handles system notifications. The web dashboard is hosted on S3 and delivered securely via CloudFront, with user authentication managed through Amazon Cognito. This architecture minimizes operational overhead and ensures high reliability for smart environment control.\nAWS Services Used AWS IoT Core: Ingests and manages MQTT data from eight smart room hubs, enabling near real-time monitoring and automation. AWS Lambda: Handles sensor data processing, configuration updates, and automation triggers (four primary functions). Amazon API Gateway: Connects the web client to backend services through secure RESTful APIs. Amazon DynamoDB: Stores user profiles, office and room configurations, and recent sensor logs for fast access. Amazon EventBridge: Triggers scheduled automation events and anomaly detection workflows. Amazon SNS: Delivers alerts and notifications when anomalies or threshold events occur. Amazon S3: Hosts the web dashboard and static assets for the Smart Office application. Amazon CloudFront: Distributes the web dashboard globally with secure HTTPS access and low latency. Amazon Cognito: Manages authentication and authorization for users interacting with the system. Amazon CloudWatch: Monitors metrics and stores operational logs for debugging and performance optimization. Component Design Sensor Hubs: Each IoT-enabled room device collects environmental data (temperature, humidity, light, etc.) and sends it to AWS IoT Core every two minutes. Data Ingestion: AWS IoT Core routes incoming MQTT messages to the SensorProcessor Lambda, which validates and logs the data into Amazon DynamoDB. Configuration Management: The RoomConfigHandler Lambda updates room settings (auto/manual modes, thresholds, timers) in DynamoDB when users make changes through the dashboard. Automation Control: AutomationSetup Lambda listens to DynamoDB Streams for configuration updates and registers automation events in Amazon EventBridge. Event Handling: EventBridge triggers AutomationHandler Lambda at scheduled times or when anomalies are detected, sending alerts via Amazon SNS if needed. User Interaction: The web dashboard (hosted on Amazon S3 and delivered via CloudFront) allows users to view sensor data and manage configurations. User Authentication: Amazon Cognito secures user access with sign-up, sign-in, and token-based authorization integrated through API Gateway. Monitoring \u0026amp; Reliability: Amazon CloudWatch collects logs and metrics from all Lambda functions, with SQS queues as DLQs to capture failed executions for debugging. 4. Technical Implementation Implementation Phases\nBuild Theory and Research AWS Services: Study core AWS components (IoT Core, Lambda, S3, API Gateway, EventBridge, CloudFront, Cognito) and learn their integration for IoT data handling and automation (7 weeks). Design Architecture and Estimate Costs: Create the serverless architecture diagram for an 8-room smart office and use the AWS Pricing Calculator to forecast monthly expenses (1 week). Develop and Optimize Solution: Implement scripts that simulate IoT sensor data to send to AWS IoT Core, build Lambda functions for automation and anomaly detection, and connect the web dashboard with API Gateway and DynamoDB (3 weeks). Testing and Deployment: Deploy all AWS services, test system reliability and rule triggers, validate message flows from IoT Core through EventBridge and Lambda, and monitor real-time results on the hosted dashboard (1 week). Technical Requirements\nSensor Hub Network: Each room includes a Sensor Hub with temperature, humidity, and light sensors, controlled by an ESP32 or similar microcontroller. Hubs send sensor data every 2 minutes via MQTT to AWS IoT Core when in auto mode. Smart Office Platform: Implements a serverless AWS architecture using AWS IoT Core (data ingestion and rule engine), AWS Lambda (data processing and automation logic), Amazon DynamoDB (device configuration and user data), Amazon S3 (web hosting), Amazon API Gateway (REST API), Amazon EventBridge (scheduled automation and anomaly detection), Amazon SNS (notifications), Amazon CloudFront (content delivery), and Amazon Cognito (user authentication). Deployment and Tools: All services are provisioned via AWS CDK/SDK. The web dashboard, built with Next.js and hosted on S3 + CloudFront, allows configuration, real-time monitoring, and automation scheduling while minimizing backend Lambda execution time. 5. Timeline \u0026amp; Milestones Project Timeline\nWeeks 1–7: Study and research AWS services including IoT Core, Lambda, DynamoDB, S3, API Gateway, and Cognito. Week 8: Design the full architecture and estimate costs using the AWS Pricing Calculator. Weeks 9–11: Develop and integrate all system components — data simulation scripts, Lambda functions, database setup, and web dashboard. Week 12: Test, debug, and deploy the system to production. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nOr you can download the Budget Estimation File.\nInfrastructure Costs AWS Services:\nAmazon DynamoDB: Free (0.00208 GB/month) AWS Lambda: Free (119,000 requests/month, 13,386.25 GB/s) AWS IoT Core: $0.18/month (8 devices, 13,000 messages/month) AWS API Gateway: Free (720 requests/month) Amazon Simple Storage Service (S3): Free (0.01 GB) Amazon CloudFront: $1.27/month (10,000 requests/month) Amazon EventBridge: Free (2600 events/month) Amazon Simple Queue Service (SQS): Free (2600 requests/month) Amazon CloudWatch: $0.25/month (0,3612736 GB/month) (set retention 3 days) Amazon Amazon Simple Notification Service (SNS): $0.02/month (2100 requests/month, 2100 emails/month) Amazon Cognito: Free (3 MAU/month) Hardware: Existing smart office sensors and hubs — no additional cost. Total: ≈ $1.81/month, or $21.72/year (within AWS Free Tier limits). 7. Risk Assessment Risk Matrix IoT Connectivity Issues: Medium impact, medium probability. Sensor Hub Malfunction: High impact, low probability. Unexpected AWS Charges: Medium impact, low probability. Data Inconsistency or Delay: Medium impact, medium probability. Mitigation Strategies Connectivity: Implement message buffering at Sensor Hub; retry on reconnect. Hardware: Keep spare hubs and perform periodic health checks. Cost: Use AWS Budgets and Cost Explorer to monitor spending within Free Tier. Data Reliability: Validate incoming IoT data via Lambda before storing in DynamoDB. Contingency Plans Switch to manual operation if IoT service disruption occurs. Enable CloudWatch alerts for early issue detection. Use AWS CloudFormation or CDK rollback for quick recovery and cost control. 8. Expected Outcomes Technical Improvements: Real-time monitoring and automation replace manual room control. Centralized platform enhances data accuracy and consistency. Scalable architecture supports future expansion to more rooms or other IoT devices. Long-term Value: Provides a reusable foundation for smart building and IoT automation research. Enables data-driven insights for energy efficiency and environmental optimization. Demonstrates a fully serverless, low-cost AWS architecture (under $2/month). "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.3-amazon-dynamodb/5.3.3-add-items/",
	"title": "Add item to tables",
	"tags": [],
	"description": "",
	"content": " In tab Tables, choose SmartOffice_User_Prod Choose Action, Create item Choose JSON view, turn off View DynamoDB JSON Enter { \u0026#34;userId\u0026#34;: \u0026#34;user-001\u0026#34;, \u0026#34;companyId\u0026#34;: \u0026#34;comp-abc\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van A\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;a@abc.com\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;offices\u0026#34;: [ \u0026#34;office-hcm\u0026#34;, \u0026#34;office-hn\u0026#34; ] } Click Create item Same for table: SmartOffice_Office_Prod { \u0026#34;officeId\u0026#34;: \u0026#34;office-hcm\u0026#34;, \u0026#34;companyId\u0026#34;: \u0026#34;comp-abc\u0026#34;, \u0026#34;officeName\u0026#34;: \u0026#34;Headquarters HCM\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;Ho Chi Minh City\u0026#34;, \u0026#34;latitude\u0026#34;: 10.762622, \u0026#34;longitude\u0026#34;: 106.660172, \u0026#34;timezone\u0026#34;: \u0026#34;Asia/Ho_Chi_Minh\u0026#34;, \u0026#34;rooms\u0026#34;: [\u0026#34;A101\u0026#34;, \u0026#34;A102\u0026#34;, \u0026#34;A103\u0026#34;, \u0026#34;A104\u0026#34;] } SmartOffice_RoomConfig_Prod { \u0026#34;roomId\u0026#34;: \u0026#34;A101\u0026#34;, \u0026#34;officeId\u0026#34;: \u0026#34;office-hcm\u0026#34;, \u0026#34;temperatureMode\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;humidityMode\u0026#34;: \u0026#34;manual\u0026#34;, \u0026#34;lightMode\u0026#34;: \u0026#34;off\u0026#34;, \u0026#34;targetTemperature\u0026#34;: 26, \u0026#34;targetHumidity\u0026#34;: 60, \u0026#34;targetLight\u0026#34;: 300, \u0026#34;lastUpdate\u0026#34;: \u0026#34;2025-10-16T14:05:00Z\u0026#34;, \u0026#34;autoOnTime\u0026#34;: \u0026#34;18:00\u0026#34;, \u0026#34;autoOffTime\u0026#34;: \u0026#34;22:00\u0026#34; } SmartOffice_RoomLog_Prod { \u0026#34;roomId\u0026#34;: \u0026#34;A101\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;1734355200\u0026#34;, \u0026#34;temperature\u0026#34;: 30, \u0026#34;humidity\u0026#34;: 70, \u0026#34;light\u0026#34;: 400, \u0026#34;expireAt\u0026#34;: 1734528000 } "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.3-amazon-dynamodb/",
	"title": "DynamoDB",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will configure Amazon DynamoDB tables to store and manage Smart Office data. Each table supports specific functions such as room configuration, user automation schedules, and real-time logs. You will also enable Time to Live (TTL) for automatic cleanup, Point-in-Time Recovery (PITR) for data protection, and DynamoDB Streams to trigger AWS Lambda functions upon data changes.\nWhy using DynamoDB with TTL, PITR, and Streams: - TTL (Time to Live) automatically removes expired items (e.g., old logs), optimizing storage and cost. - PITR (Point-in-Time Recovery) ensures data resilience, allowing full table recovery within 35 days. - Streams capture real-time updates (both old and new images) and trigger Lambda to sync room modes or automation changes with IoT Core instantly.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Jumpstart your cloud career with AWS SimuLearn The blog post “Jumpstart your cloud career with AWS SimuLearn” introduces AWS SimuLearn, an interactive learning platform that combines Generative AI with realistic customer simulations. It allows learners to build both technical and communication skills in a safe, feedback-driven environment, with training tailored by role or industry. Through the stories of Hetvi, Karishma, and Kattie, the post highlights how SimuLearn helps users strengthen customer engagement skills, deepen technical expertise, and gain industry-specific knowledge in areas like healthcare. With over 200 training modules and simulations, AWS SimuLearn empowers early-career cloud professionals to grow their AWS knowledge, boost confidence, and accelerate their cloud career readiness.\nBlog 2 - How to Set Up Automated Alerts for Newly Purchased AWS Savings Plans The blog post “How to Set Up Automated Alerts for Newly Purchased AWS Savings Plans” explains how to implement an automated monitoring and alerting system for newly purchased AWS Savings Plans with low utilization. The solution leverages AWS CloudFormation to create Step Functions, SNS topics, EventBridge schedulers, and necessary IAM roles. The deployment is split into two parts:\nMember Account: hosts a Step Function that checks Savings Plans purchased within the last 7 days and the current month, sending email alerts if utilization is below a defined threshold. Management Account: provides IAM roles and access so the Step Function can analyze Savings Plans across the organization. Users can configure thresholds, scheduling frequency, and alert recipients. Upon receiving alerts, FinOps teams should review utilization data and consider returning underused Savings Plans if eligible. This solution helps FinOps teams proactively manage cloud costs, detect underutilized commitments, and improve overall Savings Plans utilization efficiency across AWS environments. Blog 3 - Operating BYOL Windows Server Workloads Effectively on AWS This AWS blog explains how to effectively operate Bring Your Own License (BYOL) Windows Server workloads on AWS to reduce costs. It covers eligibility requirements for BYOL licenses, preparing Windows Server images using VM Import/Export and Migration Hub Orchestrator, and managing license type conversions with AWS License Manager. The post also shows how to detect configuration issues using AWS Config custom rules and analyze cost data with AWS Cost and Usage Reports (CUR). By following these best practices, organizations can optimize Windows Server costs, ensure license compliance, and improve operational efficiency on AWS.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00, October 03, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: Introduction to how AI tools such as Amazon Q Developer and Kiro transform the software development process. Focus on two key methodologies — AI-Driven Development Lifecycle (AI-DLC) emphasizing automation, speed, and scalability, and Spec-Driven Development (SDD) with Kiro emphasizing precision and structure. Presentation of the evolving role of AI in IT, where AI assists or manages development tasks while human validation remains essential.\nOutcome: Better understanding of integrating AI into software engineering while maintaining human-centered validation. Clear insight into the strengths and limitations of AI-DLC and SDD. Exposure to tools like Amazon Q Developer and Kiro, with practical applications for improving efficiency, quality, and innovation in real-world projects.\nEvent 2 Event Name:\nDate \u0026amp; Time:\nLocation:\nRole: Attendee\nDescription:\nOutcome:\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Secure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]